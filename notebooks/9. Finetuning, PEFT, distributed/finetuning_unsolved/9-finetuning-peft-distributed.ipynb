{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install peft bitsandbytes accelerate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom typing import List, Dict, Any\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.optim as optim\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, AutoConfig, set_seed\nfrom torch.utils.data import Dataset\nfrom datasets import load_dataset\nfrom peft import PeftModel, get_peft_model, LoraConfig\n\nset_seed(12, True)\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-02-18T10:18:22.533605Z","iopub.execute_input":"2025-02-18T10:18:22.534105Z","iopub.status.idle":"2025-02-18T10:18:45.110130Z","shell.execute_reply.started":"2025-02-18T10:18:22.534070Z","shell.execute_reply":"2025-02-18T10:18:45.109196Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Gradient Accumulation - 5 баллов\n\nДавайте реализуем собственную аккумуляцию градиентов.\nНиже описано обучение обычного линейного слоя. Клеткой ниже этот код скопирован, там необходимо написать аккумуляцию ргадиентов.","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ninput_size = 512\noutput_size = 256\nbatch_size = 64\ngradient_accumulation_steps = 4\n\nmodel = nn.Linear(input_size, output_size).to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nx = torch.randn(batch_size, input_size).to(device)\ny = torch.randn(batch_size, output_size).to(device)\nloss_fn = nn.MSELoss()\nfor i in range(1000):\n    optimizer.zero_grad()\n    output = model(x)\n    loss = loss_fn(output, y)\n    loss.backward()\n    optimizer.step()\n    \nprint(loss.item())","metadata":{"execution":{"iopub.status.busy":"2025-02-18T10:18:45.111751Z","iopub.execute_input":"2025-02-18T10:18:45.112371Z","iopub.status.idle":"2025-02-18T10:18:46.531568Z","shell.execute_reply.started":"2025-02-18T10:18:45.112346Z","shell.execute_reply":"2025-02-18T10:18:46.530836Z"},"trusted":true},"outputs":[{"name":"stdout","text":"1.1878371238708496\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Число шагов в аккумуляции определяется параметром gradient_accumulation_steps - это число шагов, которое мы хотим сделать перед оптимизацией.\nВам нужно поправить цикл обучения следующим образом:\n1. Разбить текущий батч на gradient_accumulation_steps частей\n2. Пройтись по каждому подбатчу (микробатчу), посчитать на нем функцию потерь, посчитать градиенты. Подумайте, нужно ли на что-либо делить или умножать функцию потерь, чтобы сохранился тот же масштаб обучения?\n3. После прохождения всех микробатчей нужно сделать шаг оптимизации","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\ninput_size = 512\noutput_size = 256\nbatch_size = 64\ngradient_accumulation_steps = 4\n\nmodel = nn.Linear(input_size, output_size).to(device)\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nx = torch.randn(batch_size, input_size).to(device)\ny = torch.randn(batch_size, output_size).to(device)\nloss_fn = nn.MSELoss()\nmini_batch_size = batch_size // gradient_accumulation_steps\nfor i in range(1000):\n    optimizer.zero_grad()\n    for batch_start in range(0, batch_size, mini_batch_size): # здесь цикл по минибатчам\n        output = model(x[batch_start:batch_start + mini_batch_size])\n        loss = loss_fn(output, y[batch_start:batch_start + mini_batch_size]) / gradient_accumulation_steps\n        loss.backward()\n    optimizer.step()\n    \nprint(loss.item()) # последний лосс такой же, как и до аккумуляции, но отмасштабированный по числу шагов (здесь 4) ","metadata":{"execution":{"iopub.status.busy":"2025-02-18T10:18:46.532688Z","iopub.execute_input":"2025-02-18T10:18:46.532947Z","iopub.status.idle":"2025-02-18T10:18:49.579909Z","shell.execute_reply.started":"2025-02-18T10:18:46.532926Z","shell.execute_reply":"2025-02-18T10:18:49.579083Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0.29420727491378784\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# QLORA - 15 баллов\nНеобходимо использовать аккумуляцию градиентов, чекпоинтинг активаций и обучение qlora.","metadata":{}},{"cell_type":"code","source":"model_name = \"NousResearch/Llama-2-7b-hf\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2025-02-18T10:18:49.580591Z","iopub.execute_input":"2025-02-18T10:18:49.580861Z","iopub.status.idle":"2025-02-18T10:18:50.949263Z","shell.execute_reply.started":"2025-02-18T10:18:49.580835Z","shell.execute_reply":"2025-02-18T10:18:50.948321Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"363734876d82444c9ec162ce91abf41e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"773c48a91e2b43ba84ea7368ae2f72fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59924885a55a47b9b1ca504db952859a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a6c33bfcfd049cf8a923b0cd265d862"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"imdb = load_dataset(\"imdb\")","metadata":{"execution":{"iopub.status.busy":"2025-02-18T10:18:50.950117Z","iopub.execute_input":"2025-02-18T10:18:50.950347Z","iopub.status.idle":"2025-02-18T10:18:56.265472Z","shell.execute_reply.started":"2025-02-18T10:18:50.950329Z","shell.execute_reply":"2025-02-18T10:18:56.264805Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03b4ea5dc10e43929b5a818436a5f7db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e8a09674c5e4ad29258d778e1376781"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a3d408e3e5b47e6876d0174ff121d35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"953555372f254425980308e63d6b07f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6dab4233bf341c7816a217774e67b8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65255249323f489d939c77481c854996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f43e04293344dcbdc1ae0d39b0b4f5"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"Наша задача научиться генерировать класс текста posive или negative, чтобы сэкономить на fewshot промпте.\n\nДавайте напишем collate_fn, которая собирает сэмпл следующим образом:\n\nесли текст имеет метку 1\n`{text} ||| posive eos`\nили\n`{text} ||| negatve eos`\nесли текст имеет метку 0. (в качестве eos можно использовать tokenizer.eos_token_id)\n\nСимволы ||| нужны нам, чтобы разделить входной текст и метку, иначе модель может не понять, что нужно генерировать метку и продолжит генерировать текст. Таким образом мы научим модель после ||| генерировать положительный или отрицательнй отзыв стоит до этого.\n\n\nВозвращать нужно словарь из 3х элементов:\n1. input_ids - LongTensor токенов. В качестве паддинга нужно использовать tokenizer.eos_token_id.\n2. attention_mask - LongTensor той же размерности, что и input_ids. 0 там, где стоят паддинги, 1 в остальных позициях\n3. labels - метки, которые мы предсказыаем. Должен быть равен -100 на всех позициях, кроме позиций, которые соответствуют метке и eos символу. \nНапример \n```python\ntokenizer.encode(\"some text ||| positive </s>\") # [1, 777, 1426, 3830, 29989, 6374, 2]\nlabels = [-100, -100, -100, -100, -100, 6374, 2]\n```\n\nТ.е. метки должны быть -100, кроме позиций, соответствующих предсказываемым токенам.","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch: List[Dict[str, Any]]):\n    class_mapping = {0: \"negative\", 1: \"positive\"}\n    texts = [sample[\"text\"] + \" ||| \" + class_mapping[sample[\"label\"]] + \" \" + tokenizer.eos_token for sample in batch]\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n    labels = inputs[\"input_ids\"].clone()\n    labels[:, :-2] = -100\n    return {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs['attention_mask'],\n        \"labels\": labels\n    }\n\nres = collate_fn([imdb[\"train\"][0], imdb[\"train\"][12505], imdb[\"train\"][2]])\n\nassert tokenizer.decode(res[\"input_ids\"][res[\"labels\"] != -100]) == \"negative</s> positive</s> negative</s>\"","metadata":{"execution":{"iopub.status.busy":"2025-02-18T10:18:56.266296Z","iopub.execute_input":"2025-02-18T10:18:56.266533Z","iopub.status.idle":"2025-02-18T10:18:56.289028Z","shell.execute_reply.started":"2025-02-18T10:18:56.266499Z","shell.execute_reply":"2025-02-18T10:18:56.288415Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Далее нам нужно создать модель в nf4, т.е. 4-битной квантизации. Конфиг уже написан, нужно лишь подать его в модель. После этого нужно:\n1. Создать конфиг адаптера LoraConfig (используйте r=8 или r=4, если будет OOM) и создать модель\n2. Создать модель с адаптером с помощью PeftModel и LoraConfig\n3. Чтобы обучение шло только по lora частям, нужно пройтись по всем параметрам модели с помощью model.named_parameters() и проставить у параметров, соответствующих lora атрибут requires_grad = True, а у всех остальных False","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_storage=torch.bfloat16,\n)\n\nmodel_name = \"NousResearch/Llama-2-7b-hf\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-18T10:18:58.939276Z","iopub.execute_input":"2025-02-18T10:18:58.939648Z","iopub.status.idle":"2025-02-18T10:20:47.912711Z","shell.execute_reply.started":"2025-02-18T10:18:58.939618Z","shell.execute_reply":"2025-02-18T10:20:47.911880Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b9fdb54f354c83a5de4b6615961f1e"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"268a4936d4454ed08ce0d85a410a11f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"309f96f26aac44a7b2e70a424fc564ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"311dbb2c8307469bb8084d0506624d0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8ef84b0d1f547bea414fba92b5b10d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc5367dd7af4b9989fe307fbd89dc00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7c4ccdb8cfa474db3dc87d8d4d812e7"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"peft_config = LoraConfig(\n    r=8,\n    lora_alpha=8,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, peft_config)\n\n# не забудьте пройтись по всем параметрам и проставить .requires_grad там, где нужно","metadata":{"execution":{"iopub.status.busy":"2025-02-18T10:20:47.913623Z","iopub.execute_input":"2025-02-18T10:20:47.913953Z","iopub.status.idle":"2025-02-18T10:20:48.062218Z","shell.execute_reply.started":"2025-02-18T10:20:47.913920Z","shell.execute_reply":"2025-02-18T10:20:48.061544Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from peft.optimizers import create_loraplus_optimizer\nimport bitsandbytes as bnb\n\n# в документации сказано, что этот оптимизатор при использовании lora ускоряет обучение\noptimizer = create_loraplus_optimizer(\n    model=model,\n    optimizer_cls=bnb.optim.Adam8bit,\n    lr=5e-5,\n    loraplus_lr_ratio=16,\n)\nscheduler = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:20:48.062968Z","iopub.execute_input":"2025-02-18T10:20:48.063204Z","iopub.status.idle":"2025-02-18T10:20:48.081404Z","shell.execute_reply.started":"2025-02-18T10:20:48.063183Z","shell.execute_reply":"2025-02-18T10:20:48.080810Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\nfor name, param in model.named_parameters():\n    if 'lora' in name.lower():\n        param.requires_grad = True\n    else:\n        param.requires_grad = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:20:48.082255Z","iopub.execute_input":"2025-02-18T10:20:48.082548Z","iopub.status.idle":"2025-02-18T10:20:48.091582Z","shell.execute_reply.started":"2025-02-18T10:20:48.082519Z","shell.execute_reply":"2025-02-18T10:20:48.090811Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"Осталось самое важное, аргументы обучения. Обязательно заполните следующие параметры:\n\n1. Батч сайз и число шагов аккумуляции выставьте так, чтобы эффективный батч сайз был 16\n2. Включите чекпоинтинг активаций","metadata":{}},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir='outputs',\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    num_train_epochs=1,\n    fp16=True,\n    report_to=None,\n    remove_unused_columns=False\n)\n\ntrainer = Trainer(\n    model=model,\n    optimizers=(optimizer, scheduler),\n    args=args,\n    train_dataset=imdb[\"train\"].select(range(2000)), # с целым датасетом 23 часа обучения, так ~3\n    tokenizer=tokenizer,\n    data_collator=collate_fn,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2025-02-18T10:20:48.092408Z","iopub.execute_input":"2025-02-18T10:20:48.092690Z","iopub.status.idle":"2025-02-18T12:35:21.475084Z","shell.execute_reply.started":"2025-02-18T10:20:48.092660Z","shell.execute_reply":"2025-02-18T12:35:21.474231Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n<ipython-input-14-608a877fe97b>:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [125/125 2:13:17, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=125, training_loss=1.869037109375, metrics={'train_runtime': 8072.79, 'train_samples_per_second': 0.248, 'train_steps_per_second': 0.015, 'total_flos': 4.78759973093376e+16, 'train_loss': 1.869037109375, 'epoch': 1.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"Давайте протестируем, что модель что-то выучила","metadata":{}},{"cell_type":"code","source":"input_text = imdb[\"test\"][0][\"text\"] + \" ||| \"\nlabel = imdb[\"test\"][0][\"label\"]\nx = tokenizer(input_text, return_tensors=\"pt\")\nfor k, v in x.items():\n    x[k] = v.cuda()\n\nprint(label)\ng = model.generate(**x, max_new_tokens=2, do_sample=False)\nprint(tokenizer.decode(g[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2025-02-18T12:47:48.152242Z","iopub.execute_input":"2025-02-18T12:47:48.152540Z","iopub.status.idle":"2025-02-18T12:47:50.443609Z","shell.execute_reply.started":"2025-02-18T12:47:48.152518Z","shell.execute_reply":"2025-02-18T12:47:50.442822Z"},"trusted":true},"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<s> I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \"Gene Roddenberry's Earth...\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again. |||  negative</s>\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"В конце вывело **negative**, значит модель дообучилась верно","metadata":{}}]}