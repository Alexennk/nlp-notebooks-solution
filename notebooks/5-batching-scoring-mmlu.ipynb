{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"11861b12","cell_type":"code","source":"from typing import List\n\nimport torch\nimport torch.nn as nn\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# можете сменить на mps на макбуке, но лично у меня он криво работает\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","metadata":{"id":"11861b12","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:03:56.991445Z","iopub.execute_input":"2025-02-08T14:03:56.991740Z","iopub.status.idle":"2025-02-08T14:04:03.017355Z","shell.execute_reply.started":"2025-02-08T14:03:56.991717Z","shell.execute_reply":"2025-02-08T14:04:03.016658Z"}},"outputs":[],"execution_count":1},{"id":"ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3","cell_type":"markdown","source":"# Знакомство с Transformers","metadata":{"id":"ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3"}},{"id":"a3df5693","cell_type":"markdown","source":"## Создание модели и предсказание следующего токена - 5 баллов\nНужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и получить следующий токен через жадную генерацию!\n\nДля загрузки модели и токенайзера вам помогут функции `.from_pretrained`\n\n**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`","metadata":{"id":"a3df5693"}},{"id":"6ec7e08b","cell_type":"code","source":"model_name = \"openai-community/gpt2\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name) # Ваш код здесь\ntokenizer = AutoTokenizer.from_pretrained(model_name) # ваш код здесь\n\ntext = \"This is a sample text\"","metadata":{"id":"6ec7e08b","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:03.018318Z","iopub.execute_input":"2025-02-08T14:04:03.018669Z","iopub.status.idle":"2025-02-08T14:04:20.229096Z","shell.execute_reply.started":"2025-02-08T14:04:03.018648Z","shell.execute_reply":"2025-02-08T14:04:20.228120Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be5474bfb564aa8a3056bb92f67398e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db1bd5803fde49cb95eb1f1d00c8240c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7343facd3f419eb0fc7ba7e11708d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfc417a7424748be95c7a817ab294a2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"311b9fe4fc59413fb74a9cbdc45ec9a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98850d2677d84177b34fa3058cd05ee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10608294da6e411f8801e01d2c630842"}},"metadata":{}}],"execution_count":2},{"id":"de25a8ea","cell_type":"code","source":"# Нужно преобразовать text с помощью tokenizer() и подать это в model.forward() (он же просто model())\n# после этого мы получим logits [batch_size = 1, seq_len, d_model]\n# По этому тензору нужно предсказать следующее слово!\nwith torch.no_grad():\n    inputs = tokenizer(text, return_tensors=\"pt\")['input_ids']\n    outputs = model(inputs)\n    logits = outputs.logits\n\n    next_token_idx: int = torch.argmax(logits[0, -1, :]).item()\n    next_token = tokenizer.decode([next_token_idx])\n\nassert next_token.strip() == \"file\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:20.230924Z","iopub.execute_input":"2025-02-08T14:04:20.231703Z","iopub.status.idle":"2025-02-08T14:04:20.385200Z","shell.execute_reply.started":"2025-02-08T14:04:20.231680Z","shell.execute_reply":"2025-02-08T14:04:20.384389Z"}},"outputs":[],"execution_count":3},{"id":"e6809813","cell_type":"markdown","source":"## Используем Generate - 5 баллов\n\nМы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\nОтличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n\nДля генерации есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n\nВаша задача написать для модели выше генерацию по тексту с:\n* Температурой - 0.9\n* Top-K - 20\n* Repetition Penalty (Frequency Penalty) - 1.2\n* максимальное число новых токенов - 10\n","metadata":{"id":"e6809813"}},{"id":"a6b62dbf","cell_type":"code","source":"text = \"This is still a sample text, but\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\nresults = []\nfor i in range(10):\n    gens = model.generate(\n        inputs['input_ids'], \n        attention_mask=inputs['attention_mask'], \n        do_sample=True,\n        max_new_tokens=10, \n        pad_token_id=tokenizer.eos_token_id,\n        temperature=0.9, \n        top_k=20, \n        repetition_penalty=1.2\n    )\n    generation: str = tokenizer.decode(gens[0]) # сгенерированный текст\n    results.append(generation)\n\nassert len(set(results)) > 1, \"Все генерации получились одинаковыми, проверьте опции генерации и флаг do_sample!\"","metadata":{"id":"a6b62dbf","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:20.386406Z","iopub.execute_input":"2025-02-08T14:04:20.386691Z","iopub.status.idle":"2025-02-08T14:04:24.005935Z","shell.execute_reply.started":"2025-02-08T14:04:20.386666Z","shell.execute_reply":"2025-02-08T14:04:24.005238Z"}},"outputs":[],"execution_count":4},{"id":"8b90512b-9420-45b3-9f4c-22fb5fa1bfc7","cell_type":"markdown","source":"## Generate Batched - 5\nТеперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n\nКогда появляется несколько текстов разной длины, то появляются и паддинги.\n\nПредставим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n\nТогда\n\n```python\ninput_ids = [\n    [3, 2, -1, -1, -1]\n    [5, 6,  7,  1,  2]\n]\nattention_mask = [\n    [1, 1, 0, 0, 0],\n    [1, 1, 1, 1, 1]\n]\n```\n\nПредставим, что мы сгенерировали еще один токен, тогда\n\n```python\ninput_ids = [\n    [3, 2, -1, -1, -1, 7]\n    [5, 6,  7,  1,  2, 8]\n]\nattention_mask = [\n    [1, 1, 0, 0, 0, 1],\n    [1, 1, 1, 1, 1, 1]\n]\n```\n\nПолучается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\nТогда исходная последовательность будет:\n\n```python\ninput_ids = [\n    [-1, -1, -1, 3, 2]\n    [ 5,  6,  7, 1, 2]\n]\nattention_mask = [\n    [0, 0, 0, 1, 1],\n    [1, 1, 1, 1, 1]\n]\n```\n\nи после генерации следующего токена\n\n```python\ninput_ids = [\n    [-1, -1, -1, 3, 2, 7]\n    [ 5,  6,  7, 1, 2, 8]\n]\nattention_mask = [\n    [0, 0, 0, 1, 1, 1],\n    [1, 1, 1, 1, 1, 1]\n]\n```\n\nВ качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n\nДля этого нам придется использовать параметр padding_side в конструкторе токенизатора.","metadata":{"id":"8b90512b-9420-45b3-9f4c-22fb5fa1bfc7"}},{"id":"5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\") # ваш код здесь\ntokenizer.pad_token_id = tokenizer.eos_token_id","metadata":{"id":"5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:24.006751Z","iopub.execute_input":"2025-02-08T14:04:24.007081Z","iopub.status.idle":"2025-02-08T14:04:24.240424Z","shell.execute_reply.started":"2025-02-08T14:04:24.007049Z","shell.execute_reply":"2025-02-08T14:04:24.239747Z"}},"outputs":[],"execution_count":5},{"id":"1938fd36","cell_type":"code","source":"# Внимание! В данном задании нужна жадная генерация!\n# Соберите оба текста в один батч и положите результаты генерации в\n# batched_generations\ntexts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\ninputs = tokenizer(texts, return_tensors=\"pt\", padding=True)\n\nbatched_generations: List[str] = []\ngenerations = model.generate(\n    inputs['input_ids'], \n    attention_mask=inputs['attention_mask'], \n    max_new_tokens=10, \n    do_sample=False, # для жадной генерации\n    pad_token_id=tokenizer.eos_token_id,\n)\nbatched_generations = tokenizer.batch_decode(generations) # декодирую в строку, потому что в ассерте идет сравнение по \"==\" и для тензоров оно не работает\n\n# Пройдитесь по каждому сэмплу по отдельности и положите результаты генерации\n# в single_generations\nsingle_generations: List[str] = []\nfor i in range(len(texts)):\n    gens = model.generate(\n        inputs['input_ids'][i].unsqueeze(0), \n        attention_mask=inputs['attention_mask'][i].unsqueeze(0), \n        max_new_tokens=10, \n        do_sample=False, # для жадной генерации\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    generation: str = tokenizer.decode(gens[0])\n    single_generations.append(generation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:24.241199Z","iopub.execute_input":"2025-02-08T14:04:24.241478Z","iopub.status.idle":"2025-02-08T14:04:25.448167Z","shell.execute_reply.started":"2025-02-08T14:04:24.241451Z","shell.execute_reply":"2025-02-08T14:04:25.447423Z"}},"outputs":[],"execution_count":6},{"id":"3b9c8911","cell_type":"code","source":"assert len(batched_generations) == 2 and len(single_generations) == 2\nfor s, b in zip(batched_generations, single_generations):\n    assert s == b","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:25.448961Z","iopub.execute_input":"2025-02-08T14:04:25.449268Z","iopub.status.idle":"2025-02-08T14:04:25.453029Z","shell.execute_reply.started":"2025-02-08T14:04:25.449241Z","shell.execute_reply":"2025-02-08T14:04:25.452149Z"}},"outputs":[],"execution_count":7},{"id":"f5da008c-3653-40d5-89ba-cd831352fd3d","cell_type":"markdown","source":"# Скоринг, Perplexity - 10 баллов\n\nМожно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n\nОпустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n\nТогда вероятность этого текста расписывается через произведение условных вероятностей:\n\n```\nP(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n```\n\nТ.е. это вероятность слова при условии его левого контекста.\nЗачастую ее обозначают как $P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\nЭти вероятности можно взять из выходного вектора!\n\nДавайте попробуем подсчитать вероятность и perplexity текстов!\nperplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n\n$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n\nВ этом задании нужно:\n1. Посчитать вероятность **text**\n2. Посчитать перплексию **text**\n\nЕще одна важная деталь:\nработать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\nДля этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n\n$$ p = p_1 * p_2 * p_3 $$\n$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n\nВ pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!","metadata":{"id":"f5da008c-3653-40d5-89ba-cd831352fd3d"}},{"id":"e1c7ba39-a451-43a2-ac55-629c99259abe","cell_type":"code","source":"print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\nprint(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\ntext = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n\ninputs = tokenizer(text, return_tensors=\"pt\")['input_ids']","metadata":{"id":"e1c7ba39-a451-43a2-ac55-629c99259abe","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:25.454914Z","iopub.execute_input":"2025-02-08T14:04:25.455176Z","iopub.status.idle":"2025-02-08T14:04:25.468438Z","shell.execute_reply.started":"2025-02-08T14:04:25.455149Z","shell.execute_reply":"2025-02-08T14:04:25.467782Z"}},"outputs":[{"name":"stdout","text":"Beginning of sentence (BOS) token = `<|endoftext|>`\nEnd of sentence (EOS) token  = `<|endoftext|>`\n","output_type":"stream"}],"execution_count":8},{"id":"9c73b0b2","cell_type":"code","source":"with torch.no_grad():\n    logits = model(inputs).logits # [batch_size=1, seq_len=9, d_model=50257]\n    # ваш код здесь!\n    # 1. Нужно обрезать logits по длине, т.к. для предсказаний по последнему токену нечего считать\n    logits = logits[:, :-1, :]\n    # 2. Превращаем logits в log_probs\n    log_probs = torch.log_softmax(logits, dim=-1)\n    # 3. Берем вероятности следующих токенов, т.к. по вектору i-й позиции мы предсказываем токен на позиции (i + 1)\n    # для этого нам поможет torch.gather\n    targets = inputs[:, 1:]\n    log_probs_next_token = torch.gather(log_probs, dim=-1, index=targets.unsqueeze(-1)) # вероятности для токенов из targets\n    # 4. Считаем вероятности и perplexity!\n    print(\"Text probability:\", log_probs_next_token.sum().exp().item())\n    print(\"Perplexity:\", (-log_probs_next_token.mean()).exp().item())\n\n# должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:25.469450Z","iopub.execute_input":"2025-02-08T14:04:25.469707Z","iopub.status.idle":"2025-02-08T14:04:25.556838Z","shell.execute_reply.started":"2025-02-08T14:04:25.469688Z","shell.execute_reply":"2025-02-08T14:04:25.556186Z"}},"outputs":[{"name":"stdout","text":"Text probability: 2.1782111230277297e-14\nPerplexity: 51.0196533203125\n","output_type":"stream"}],"execution_count":9},{"id":"4f244eac-7cb1-4689-8adc-46662891e657","cell_type":"markdown","source":"# Вопросы - 5 баллов\n\n**Ответьте на вопрсоы текстом прямо здесь!**\n\n\n1. Какое значение P(X) вероятности текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n2. Какое значение перплексии текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n\n","metadata":{"id":"4f244eac-7cb1-4689-8adc-46662891e657"}},{"id":"bc446a65","cell_type":"markdown","source":"**Ответы:**\n1. Близкое к единице: тогда у каждой из условных вероятностей $P(x_i|x_{<i})$ значение будет также ~1 и практически наверняка при генерации будет выбран соответствующий токен.\n2. Чем ниже perplexity, тем увереннее модель. В идеале это значение равное 1","metadata":{}},{"id":"5ddd5038-620b-48bb-bbc1-db3729141d78","cell_type":"markdown","source":"# Chat-Models","metadata":{"id":"5ddd5038-620b-48bb-bbc1-db3729141d78"}},{"id":"599c7530-a7ce-4d23-abaf-2b0cec87301e","cell_type":"markdown","source":"# Формат - 5 баллов\nКак мы обсуждали на лекции, все chat-модели принимают входы в своем особом формате.\nОн может быть описан текстом, а может быть заложен в шаблон, который доступен через `tokenizer.apply_chat_template`","metadata":{"id":"599c7530-a7ce-4d23-abaf-2b0cec87301e"}},{"id":"d95e45be-4c3a-4be8-9720-8b8c05188927","cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:25.557577Z","iopub.execute_input":"2025-02-08T14:04:25.557835Z","iopub.status.idle":"2025-02-08T14:04:25.561363Z","shell.execute_reply.started":"2025-02-08T14:04:25.557801Z","shell.execute_reply":"2025-02-08T14:04:25.560594Z"}},"outputs":[],"execution_count":10},{"id":"7f5fe593-63a8-406d-9678-6d805c180670","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\")\nmodel = AutoModelForCausalLM.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.half).to(device)\n# модель на 15.3 ГБ видеопамяти, там дальше было нелегко","metadata":{"id":"7f5fe593-63a8-406d-9678-6d805c180670","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:04:25.561936Z","iopub.execute_input":"2025-02-08T14:04:25.562252Z","iopub.status.idle":"2025-02-08T14:12:00.428622Z","shell.execute_reply.started":"2025-02-08T14:04:25.562227Z","shell.execute_reply":"2025-02-08T14:12:00.427877Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc4b0ab1f6794f0493802119446e93aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ece62dea7c74f32b79c0bf894d644a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4afb789b417e4232a13594cc8e3e5d62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d8766b1bf8a46f79dde1ae09b0cfe12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc69ed13093f47cfacd0884adc711c38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fed98582de9c411a899f26b8d0eb85cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"397d8e5879a74f2b8e3c72b3510c2fa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95875daf51f64820892b04592832c5b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60241c339a6540f0baba9dfa9fc50fa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"932b578483474ea9bdaefb3d79fe587f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a677d45c8d5445f2bf807b0e7249e645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"349b4840d2b140b8b4d80a32798be4a2"}},"metadata":{}}],"execution_count":11},{"id":"503f15fd-576a-4e8e-917a-74df01a944f4","cell_type":"markdown","source":"Давайте посмотрим, как chat модель отработает на обычном тексте. Используйте для генерации сэмплинг и kv cache, выведите 5 результатов генерации.","metadata":{"id":"503f15fd-576a-4e8e-917a-74df01a944f4"}},{"id":"7134f0bb-1ee4-4508-a26d-5326ea96562b","cell_type":"code","source":"text = \"hello how are you\"\ninputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\nfor i in range(5):\n    with torch.no_grad():\n        gens = model.generate(\n            inputs['input_ids'], \n            attention_mask=inputs['attention_mask'], \n            do_sample=True,\n            max_new_tokens=20, \n            pad_token_id=tokenizer.eos_token_id,\n        )\n        generated_text: str = tokenizer.decode(gens[0]) # сгенерированный текст\n        print(generated_text)\n        print(\"====\" * 3)","metadata":{"id":"7134f0bb-1ee4-4508-a26d-5326ea96562b","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T12:36:13.970517Z","iopub.execute_input":"2025-02-07T12:36:13.970834Z","iopub.status.idle":"2025-02-07T12:36:19.997034Z","shell.execute_reply.started":"2025-02-07T12:36:13.970805Z","shell.execute_reply":"2025-02-07T12:36:19.996327Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|>hello how are you doing today?\nI'm doing well, thanks for asking! It's great to connect with you.\n============\n<|begin_of_text|>hello how are you?\nI'm a bot, so I don't have feelings like humans do, but I'm here\n============\n<|begin_of_text|>hello how are you?\nI'm doing well, thanks for asking! It's great to hear from you. How about\n============\n<|begin_of_text|>hello how are you? i am so excited to be here and to share my story with you. my name is em\n============\n<|begin_of_text|>hello how are you?\nI'm doing well, thanks for asking! It's great to connect with you. How about\n============\n","output_type":"stream"}],"execution_count":16},{"id":"3fd50470-64b9-4a21-8748-0e9c5ea439fc","cell_type":"markdown","source":"Видим, что текст зачастую выходит мусорный. Это потому что формат входных данных сильно отличается от того, что модель видела на обучении.\nКак мы уже обсуждали, у всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели.\n\nНе забудьте про опцию add_generation_prefix - она добавляет часть формата, после которой ожидается ответ модели!","metadata":{"id":"3fd50470-64b9-4a21-8748-0e9c5ea439fc"}},{"id":"88def36d-ea36-47fb-8b34-63f0b8ed0c8d","cell_type":"code","source":"messages = [\n    {\"role\": \"user\", \"content\": \"hello\"},\n    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n    {\"role\": \"user\", \"content\": \"I love you\"},\n]\n\nprefix = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n# с add_generation_prefix не очень работало, а add_generation_prompt подошел","metadata":{"id":"e79a3701-c80f-4b90-90bd-fa010e32ea36","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:31:10.915308Z","iopub.execute_input":"2025-02-07T14:31:10.915640Z","iopub.status.idle":"2025-02-07T14:31:10.924594Z","shell.execute_reply.started":"2025-02-07T14:31:10.915611Z","shell.execute_reply":"2025-02-07T14:31:10.924007Z"}},"outputs":[],"execution_count":14},{"id":"2f2e4e51-24d4-4f16-9c7b-2663781c2c01","cell_type":"code","source":"reference = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nhello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nI'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nI love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\nassert prefix.strip() == reference.strip()","metadata":{"id":"e79a3701-c80f-4b90-90bd-fa010e32ea36","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:31:15.429129Z","iopub.execute_input":"2025-02-07T14:31:15.429418Z","iopub.status.idle":"2025-02-07T14:31:15.433062Z","shell.execute_reply.started":"2025-02-07T14:31:15.429394Z","shell.execute_reply":"2025-02-07T14:31:15.432213Z"}},"outputs":[],"execution_count":15},{"id":"048b8882-bec0-4bf4-b6fb-30e727d095c6","cell_type":"markdown","source":"Давайте посмотрим, что нам ответит модель!","metadata":{"id":"048b8882-bec0-4bf4-b6fb-30e727d095c6"}},{"id":"4284b18d-4f9b-4e7d-b3ea-bb365e90093c","cell_type":"code","source":"inputs = tokenizer(prefix, return_tensors=\"pt\").to(device)\nwith torch.no_grad():\n    gens = model.generate(\n        inputs['input_ids'], \n        attention_mask=inputs['attention_mask'], \n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id,\n    )\n    generated_text: str = tokenizer.decode(gens[0]) # сгенерированный текст\n    print(generated_text)","metadata":{"id":"4284b18d-4f9b-4e7d-b3ea-bb365e90093c","trusted":true,"execution":{"iopub.status.busy":"2025-02-07T14:31:19.470752Z","iopub.execute_input":"2025-02-07T14:31:19.471062Z","iopub.status.idle":"2025-02-07T14:31:22.039160Z","shell.execute_reply.started":"2025-02-07T14:31:19.471037Z","shell.execute_reply":"2025-02-07T14:31:22.038188Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nhello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nI'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nI love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nAw, thank you so much! That's very kind of you to say! I'm happy to be here and help you in any way I can.<|eot_id|>\n","output_type":"stream"}],"execution_count":16},{"id":"a72482f3-c296-46f3-851c-57b4f91a717b","cell_type":"markdown","source":"## Benchmark - 15","metadata":{"id":"a72482f3-c296-46f3-851c-57b4f91a717b"}},{"id":"52f422a9-c2ee-4c17-8aee-1830f1d143e6","cell_type":"markdown","source":"Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n* question - вопрос\n* choices - варианты ответа\n* answer - номер правильного ответа","metadata":{"id":"52f422a9-c2ee-4c17-8aee-1830f1d143e6"}},{"id":"530d1721-6623-4ca6-816c-d4f90203ceb2","cell_type":"code","source":"from datasets import load_dataset\nmmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\nmmlu[1]","metadata":{"id":"530d1721-6623-4ca6-816c-d4f90203ceb2","outputId":"737e99f7-c31e-4a94-9df4-a881ea514666","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:14:15.088940Z","iopub.execute_input":"2025-02-08T14:14:15.089298Z","iopub.status.idle":"2025-02-08T14:14:19.345377Z","shell.execute_reply.started":"2025-02-08T14:14:15.089273Z","shell.execute_reply":"2025-02-08T14:14:19.344593Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/53.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"822afb18c77f47c6896971478c8a5862"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dataset_infos.json:   0%|          | 0.00/138k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b52919a2d9834732a195f6c0bc2b13fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/11.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d04829ec159465a9589d69698a3719e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"544efb505d754e169cf29f1ea147671d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dev-00000-of-00001.parquet:   0%|          | 0.00/3.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cbb67921eac4545855a8977a85b4130"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d4defe7a0794b68b1ff1d125eeaff36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16c535dfa47d4a1ba6ca8f1a457ed061"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/5 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f53b4fadbc5146328616759ed7ae7556"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'question': 'What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices?',\n 'subject': 'global_facts',\n 'choices': ['About $300', 'About $3k', 'About $8k', 'About $15k'],\n 'answer': 1}"},"metadata":{}}],"execution_count":12},{"id":"fca61a91-5784-44f3-af9b-72250f8d58a4","cell_type":"markdown","source":"Наша задача здесь решить задачу многоклассовой классификации.\nДля этого нужно посчитать\n$$P(choices_i | question)$$\nт.е. для посчитать вероятность каждого варианта ответа для вопроса. Мы это уже делали кодом выше!\n\nПосле этого давайте брать самый вероятный ответ и считать, что модель его выбрала.\nПосле этого давайте посчитаем accurracy, т.е. долю правильных ответов.\nВместо вероятностей для подсчета лучше использовать logprobs.\n\nИтого, что нужно сделать:\n1. Пройтись по датасету, для каждого question и каждого из соответствующих choices получить самый вероятный ответ.\n2. Посчитать итоговый accuracy\n\n**Важно**\n1. Выше мы уже написали скоринг текста с помощью LLM, для этого задания можно адаптировать функцию.\n2. Если делаете варианты с батчеванием помните: длины choices могут быть разными! Нужно не считать вероятности по паддингам. В этом нам помогут attention_masks из выходов `tokenizer()`\n3. В данном задании для простоты мы игнорируем формат ответа llama3 и делаем скоринг по f\"{question} {answer}\"\n\n\nПопробуйте для начала написать вариант со скорингом для батча размера 1, а потом для батча размера 3 или 5. Код должен корректно работать для батча любого размера и выдавать одинаковую итоговую точность.\n\nЗа задание, в котором код работает только с батчом размера 1, 2, 4 можно получить **только 10 баллов**","metadata":{"id":"fca61a91-5784-44f3-af9b-72250f8d58a4"}},{"id":"4a4126e2-c463-404d-a3b5-5361f744242e","cell_type":"code","source":"def sample_to_texts(sample):\n    return [sample[\"question\"] + \" \" + answer for answer in sample[\"choices\"]]\n\nall_samples_formatted = sum([sample_to_texts(sample) for sample in mmlu], [])\nprint(*all_samples_formatted[2:6], sep=\"\\n\")\n# ваш код здесь! (чуть ниже на самом деле)","metadata":{"id":"4a4126e2-c463-404d-a3b5-5361f744242e","outputId":"3bd51c60-2716-4a18-9c58-b2dff1ef9319","trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:14:24.294510Z","iopub.execute_input":"2025-02-08T14:14:24.295136Z","iopub.status.idle":"2025-02-08T14:14:24.306817Z","shell.execute_reply.started":"2025-02-08T14:14:24.295104Z","shell.execute_reply":"2025-02-08T14:14:24.306086Z"}},"outputs":[{"name":"stdout","text":"As of 2016, about what percentage of adults aged 18 years or older were overweight? 40%\nAs of 2016, about what percentage of adults aged 18 years or older were overweight? 80%\nWhat was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices? About $300\nWhat was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices? About $3k\n","output_type":"stream"}],"execution_count":13},{"id":"b9d7e6cd-f292-484f-be4d-e52acd420c11","cell_type":"code","source":"from tqdm.notebook import tqdm\ntokenizer.pad_token = tokenizer.eos_token\n\nmax_seq_length = max(len(tokenizer(item)['input_ids']) for item in all_samples_formatted)\nmax_seq_length","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:20:38.652832Z","iopub.execute_input":"2025-02-08T14:20:38.653320Z","iopub.status.idle":"2025-02-08T14:20:38.712225Z","shell.execute_reply.started":"2025-02-08T14:20:38.653275Z","shell.execute_reply":"2025-02-08T14:20:38.711343Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"117"},"metadata":{}}],"execution_count":18},{"id":"b3447965-f2fe-4b8c-ba79-8a5b45f1cf53","cell_type":"code","source":"def score_batch(questions, choices):\n    \"\"\"\n    Функция для подсчета logprobs всех вариантов ответа\n    \"\"\"\n    batch_inputs = []\n    choices_lens = [] # это для передачи обратно в evaluate_accuracy\n    for question, choice_set in zip(questions, choices):\n        batch_inputs.extend([f\"{question} {choice}\" for choice in choice_set])\n        choices_lens.append(len(choice_set))\n\n    # здесь все тексты дополнятся паддингами слева до длины \"max_seq_length\"\n    encodings = tokenizer(batch_inputs, padding=\"max_length\", max_length=max_seq_length, truncation=True, return_tensors=\"pt\")\n    input_ids = encodings[\"input_ids\"].to(device)\n    attention_mask = encodings[\"attention_mask\"].to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n\n    # считаю логарифмическую вероятность последнего токена каждого ответа, код очень похож на секцию со скорингом чуть выше\n    logprobs_per_answer = []\n    for i in range(len(batch_inputs)):\n        choice_tokens = input_ids[i, 1:] # это по сути targets\n        mask = attention_mask[i, 1:].bool() # маска для учета паддингов\n        log_probs_next_token = torch.gather(log_probs[i, :-1, :], dim=-1, index=choice_tokens.unsqueeze(-1))\n        valid_log_probs = log_probs_next_token[mask] # тут отбрасываются вероятности для паддинг-токенов\n        logprobs_per_answer.append(valid_log_probs.sum().item()) # собираются вероятности ответов\n\n    return logprobs_per_answer, choices_lens\n\ndef evaluate_accuracy(batch_size=1):\n    correct = 0\n    total = 0\n\n    for i in tqdm(range(0, len(mmlu), batch_size)):\n        batch = mmlu[i : i + batch_size]\n\n        questions = [item for item in batch['question']]\n        choices = [item for item in batch['choices']]\n        correct_answers = [item for item in batch['answer']]\n\n        log_probs, choices_lens = score_batch(questions, choices)\n\n        # тут получаются номера предсказаний\n        pred_indices = []\n        start_idx = 0\n        for j in range(batch_size):\n            end_idx = start_idx + choices_lens[j] # чтобы ниже отобрать предсказанные вероятности для одного вопроса\n            pred_indices.append(torch.tensor(log_probs[start_idx:end_idx]).argmax().item()) # номер выбранного моделью ответа\n            start_idx = end_idx\n\n        # для подсчета accuracy\n        correct += sum(p == a for p, a in zip(pred_indices, correct_answers))\n        total += batch_size\n\n    return correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:26:30.331059Z","iopub.execute_input":"2025-02-08T14:26:30.331384Z","iopub.status.idle":"2025-02-08T14:26:30.340188Z","shell.execute_reply.started":"2025-02-08T14:26:30.331360Z","shell.execute_reply":"2025-02-08T14:26:30.339296Z"}},"outputs":[],"execution_count":19},{"id":"792a70f3-0601-48d2-a0f3-7dd7a208cb75","cell_type":"markdown","source":"**Пару слов насчет реализации обработки choices разных длин.** Я пробовал:\n- просто не перемножать вероятности для токенов-паддингов при подсчете вероятности текста.\nЭто не помогает полностью, так как из-за разных длин входного текста софтмакс будет давать разные вероятности и в итоге произведение тоже будет разным для разных batch size\n- в логитах заменять соответствующие паддингам значения на -inf, чтобы софтмакс дал на этом месте 0. \nНо он давал NaN\n- явно в log_probs проставлять 1 на месте паддинга. Это и не помогло, и к тому же вероятности уже не сложились бы в единицу\n- решил при токенизации проставить padding=\"max_length\" и добавлять паддинги до макс. длины (это 102) для каждого текста\nНо опять же, надеюсь, что бывает и более лаконичное решение, при котором разные размеры батча не влияли бы на accuracy","metadata":{}},{"id":"7fe71f22-31ac-4101-92c8-07a8ffb019f8","cell_type":"code","source":"accuracy_1 = evaluate_accuracy(batch_size=1)\n\nprint(f\"Accuracy (batch=1): {accuracy_1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T13:54:49.577878Z","iopub.execute_input":"2025-02-08T13:54:49.578216Z","iopub.status.idle":"2025-02-08T13:56:40.160547Z","shell.execute_reply.started":"2025-02-08T13:54:49.578187Z","shell.execute_reply":"2025-02-08T13:56:40.158345Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea08faffc3884a4795783152f4a447cc"}},"metadata":{}},{"name":"stdout","text":"Accuracy (batch=1): 0.4900\n","output_type":"stream"}],"execution_count":20},{"id":"ba700216-e127-448d-b3eb-17716e19cbe9","cell_type":"code","source":"accuracy_2 = evaluate_accuracy(batch_size=2)\n\nprint(f\"Accuracy (batch=2): {accuracy_2:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T14:14:38.963854Z","iopub.execute_input":"2025-02-08T14:14:38.964199Z","iopub.status.idle":"2025-02-08T14:16:28.117057Z","shell.execute_reply.started":"2025-02-08T14:14:38.964172Z","shell.execute_reply":"2025-02-08T14:16:28.116076Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e382ee83f34984a74ae7c3cda99bc3"}},"metadata":{}},{"name":"stdout","text":"Accuracy (batch=2): 0.4900\n","output_type":"stream"}],"execution_count":17},{"id":"a3f2c8d4-d35e-431e-abe9-5ea2b224fbf0","cell_type":"markdown","source":"С учетом классификации на 4+ классов вполне неплохо. А что касается размера батча, то мне не хватало видеопамяти на batch_size > 2. Я тестировал на более простой модели и значение accuracy остается неизменным","metadata":{}},{"id":"8a86188a","cell_type":"markdown","source":"# Вопросы - 5 баллов","metadata":{"id":"8a86188a"}},{"id":"76c440af-dfc0-460d-a113-3fab3fefa361","cell_type":"markdown","source":"**Ответьте на следующие вопросы (5 баллов в сумме)**:\n1. Как влияет длина ответа на вероятность ответа при скоринге? Если есть какие-либо проблемы, как бы вы с этим боролись.\n2. Если к началу каждого ответа добавилить метки A) B) C) D) станет ли модель отвечать лучше или хуже?\nСтоит ли по-вашему добавлять эти метки?\n","metadata":{"id":"76c440af-dfc0-460d-a113-3fab3fefa361"}},{"id":"23e54b2c-ea26-4076-aef0-71bc53b1e5ef","cell_type":"markdown","source":"**Ответы:**\n1. Суммарная вероятность более длинного ответа может быть чуть ниже за счет произведения большего числа значений из [0, 1]. Поэтому если у правильного ответа будет более длинная формулировка, чем у более коротких, шансы на выбор моделью правильного ответа станут чуть ниже. Из решение самое действенное это, полагаю, использовать логарифмы вероятностей и производить вычисления с ними. Также можно попробовать домножать итоговые вероятности текста на какое-нибудь обратно-пропорциональное его длине значение\n2. Я предполагаю, что от добавления меток пользы много не будет, потому что информации в себе они не несут и могут в каком-то смысле запутать модель. Думаю, что ожидать помощи от них можно только если данные, на которых обучалась модель, были похожим образом структурированы","metadata":{}}]}