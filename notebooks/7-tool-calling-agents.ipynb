{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"32ae2216-226c-4dd6-9b16-df9184c17b0b","cell_type":"code","source":"!pip install together","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"33c44059","cell_type":"code","source":"import json\nfrom typing import List, Dict\n\nimport requests\nfrom together import Together\nfrom transformers import AutoTokenizer","metadata":{"id":"33c44059","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:27:44.486060Z","iopub.execute_input":"2025-02-10T16:27:44.486422Z","iopub.status.idle":"2025-02-10T16:27:55.413743Z","shell.execute_reply.started":"2025-02-10T16:27:44.486388Z","shell.execute_reply":"2025-02-10T16:27:55.412840Z"}},"outputs":[],"execution_count":3},{"id":"782f4847","cell_type":"markdown","source":"# Предобработка входных данных\n\nВ данном задании мы будем ходить в онлайн модель. Предлагается все также ходить в together.ai, т.к. они дают $5 кредита при регистрации.\n\nВначале давайте руками поиграемся с API, посмотрим, как походы в API соотносятся с тем, что мы делали в домашнем задании \"Доступные LLM\"","metadata":{"id":"782f4847"}},{"id":"c3ca6131","cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3.1-70B\")","metadata":{"id":"c3ca6131","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:27:59.815498Z","iopub.execute_input":"2025-02-10T16:27:59.816094Z","iopub.status.idle":"2025-02-10T16:28:02.288343Z","shell.execute_reply.started":"2025-02-10T16:27:59.816054Z","shell.execute_reply":"2025-02-10T16:28:02.287461Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1d3b43391cf447cbdeb894fd9555702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f728092d2024229b9abc5033a8cb1b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f06bf6c465044ae947fcf2403c03296"}},"metadata":{}}],"execution_count":4},{"id":"c2a9ce64","cell_type":"code","source":"tokenizer.bos_token, tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:06.539461Z","iopub.execute_input":"2025-02-10T16:28:06.539781Z","iopub.status.idle":"2025-02-10T16:28:06.545580Z","shell.execute_reply.started":"2025-02-10T16:28:06.539756Z","shell.execute_reply":"2025-02-10T16:28:06.544628Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"('<|begin_of_text|>', '<|end_of_text|>')"},"metadata":{}}],"execution_count":5},{"id":"58bfcc32","cell_type":"code","source":"tokenizer.start_header_token = '<|start_header_id|>'\ntokenizer.end_header_token = '<|end_header_id|>'\ntokenizer.eot_token = '<|eot_id|>'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:07.882180Z","iopub.execute_input":"2025-02-10T16:28:07.882527Z","iopub.status.idle":"2025-02-10T16:28:07.886877Z","shell.execute_reply.started":"2025-02-10T16:28:07.882499Z","shell.execute_reply":"2025-02-10T16:28:07.885706Z"}},"outputs":[],"execution_count":6},{"id":"6c702b9d","cell_type":"markdown","source":"## Ручное форматирование промпта - 5 баллов","metadata":{"id":"6c702b9d"}},{"id":"f0ad05b7","cell_type":"markdown","source":"Давайте попробуем собрать вход для llama3.1 руками, для этого допишем функцию `format_messages_to_prompt`.\nОна принимает messages - массив словарей, где указаны роли и текст сообщений, а возвращает она текст в формате, который нужно подать модели.\n\nНапример для истории сообщений\n\n```python\nmessages = [\n    {\"role\": \"system\", \"content\": \"Some system message\"},\n    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n]\n```\n\nдолжен выдаваться итоговый промпт\n\n```text\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nSome system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThis is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nthis is a mesage from the assistant<|eot_id|>\n```\n\nЧто важно:\n1. Текст начинается со спецтокена bos\n2. Дальше идет заголовок start_header_id + end_header_id, которые содержат роль\n3. Дальше после \\n\\n идет текст, заканчивающийся на eot_id\n4. Дальше следующий заголовок с новой ролью и т.д.\n\n**Важно** - в данной функции нельзя использовать `tokenizer.apply_chat_template`","metadata":{"id":"f0ad05b7"}},{"id":"f0fb161e","cell_type":"code","source":"def format_messages_to_prompt(messages: List[Dict[str, str]]) -> str:\n    prompt = f\"{tokenizer.bos_token}\"\n    for message in messages:\n        prompt += f\"\"\"{tokenizer.start_header_token}{message['role']}{tokenizer.end_header_token}\n\n{message['content']}{tokenizer.eot_token}\"\"\"\n    return prompt\n\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"Some system message\"},\n    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n    {\"role\": \"assistant\", \"content\": \"this is a mesage from the assistant\"}\n]","metadata":{"id":"f0fb161e","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:10.676899Z","iopub.execute_input":"2025-02-10T16:28:10.677227Z","iopub.status.idle":"2025-02-10T16:28:10.682757Z","shell.execute_reply.started":"2025-02-10T16:28:10.677202Z","shell.execute_reply":"2025-02-10T16:28:10.681736Z"}},"outputs":[],"execution_count":7},{"id":"b8d7481f","cell_type":"code","source":"reference_text = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nSome system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThis is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nthis is a mesage from the assistant<|eot_id|>\"\"\"\n\n\nassert format_messages_to_prompt(messages) == reference_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:12.903829Z","iopub.execute_input":"2025-02-10T16:28:12.904208Z","iopub.status.idle":"2025-02-10T16:28:12.908702Z","shell.execute_reply.started":"2025-02-10T16:28:12.904180Z","shell.execute_reply":"2025-02-10T16:28:12.907533Z"}},"outputs":[],"execution_count":8},{"id":"3a536107","cell_type":"markdown","source":"Мы также помним, что раньше у нас была `tokenizer.apply_chat_template`. Т.к. у нас неофициальный форк llama3.1, то chat_template в токенайзер нам не завезли, поэтому придется добавить его руками","metadata":{"id":"3a536107"}},{"id":"3e5db895","cell_type":"code","source":"chat_template = \"\"\"\n{{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n\"\"\".strip()\ntokenizer.chat_template = chat_template","metadata":{"id":"3e5db895","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:15.163989Z","iopub.execute_input":"2025-02-10T16:28:15.164361Z","iopub.status.idle":"2025-02-10T16:28:15.169835Z","shell.execute_reply.started":"2025-02-10T16:28:15.164328Z","shell.execute_reply":"2025-02-10T16:28:15.168960Z"}},"outputs":[],"execution_count":9},{"id":"18ea72a3","cell_type":"markdown","source":"## Автоматическая сборка промпта - 5 баллов","metadata":{"id":"18ea72a3"}},{"id":"32fe9b9f","cell_type":"markdown","source":"Давайте вспомним теперь на деле, как используется chat_template! Попробуем использовать функцию `tokenizer.apply_chat_template`","metadata":{"id":"32fe9b9f"}},{"id":"7e493aae","cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"Some system message\"},\n    {\"role\": \"user\", \"content\": \"This is a message from the user\"},\n    {\"role\": \"assistant\", \"content\": \"this is a message from the assistant\"}\n]\n\nprompt = tokenizer.apply_chat_template(chat_template=tokenizer.chat_template, conversation=messages, tokenize=False) # Ваш код здесь","metadata":{"id":"7e493aae","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:17.958658Z","iopub.execute_input":"2025-02-10T16:28:17.959064Z","iopub.status.idle":"2025-02-10T16:28:17.991596Z","shell.execute_reply.started":"2025-02-10T16:28:17.959023Z","shell.execute_reply":"2025-02-10T16:28:17.990828Z"}},"outputs":[],"execution_count":10},{"id":"c32bbca6","cell_type":"code","source":"print(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:20.306013Z","iopub.execute_input":"2025-02-10T16:28:20.306362Z","iopub.status.idle":"2025-02-10T16:28:20.311111Z","shell.execute_reply.started":"2025-02-10T16:28:20.306333Z","shell.execute_reply":"2025-02-10T16:28:20.309845Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nSome system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThis is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nthis is a message from the assistant<|eot_id|>\n","output_type":"stream"}],"execution_count":11},{"id":"23f913d5","cell_type":"code","source":"print(tokenizer.apply_chat_template(chat_template=tokenizer.chat_template, conversation=messages, tokenize=False, add_generation_prompt=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:22.923360Z","iopub.execute_input":"2025-02-10T16:28:22.923688Z","iopub.status.idle":"2025-02-10T16:28:22.928809Z","shell.execute_reply.started":"2025-02-10T16:28:22.923664Z","shell.execute_reply":"2025-02-10T16:28:22.927969Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nSome system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThis is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nthis is a message from the assistant<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n","output_type":"stream"}],"execution_count":12},{"id":"fee33f0b","cell_type":"code","source":"reference_prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nSome system message<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThis is a message from the user<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nthis is a message from the assistant<|eot_id|>\"\"\"\n\nassert prompt == reference_prompt","metadata":{"id":"fee33f0b","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:25.211438Z","iopub.execute_input":"2025-02-10T16:28:25.211740Z","iopub.status.idle":"2025-02-10T16:28:25.215757Z","shell.execute_reply.started":"2025-02-10T16:28:25.211717Z","shell.execute_reply":"2025-02-10T16:28:25.214790Z"}},"outputs":[],"execution_count":13},{"id":"ed104b62","cell_type":"markdown","source":"Обратите внимание, что в заданном chat_template указаны Cutting Knowledge Date, т.е. до данные до какого периода видела модели, и Today Date - захардкоженная дата текущего диалога.\n\n**Вопрос, обязательно напишите свой ответ здесь!**\nНа что влияет аргумент `add_generation_prompt` в функции `tokenizer.apply_chat_template`? Зачем его использовать?","metadata":{"id":"ed104b62"}},{"id":"7ed0a49b","cell_type":"markdown","source":"**Ответ**:\nВ конце форматированного промпта будет добавлен текст начала ответа ассистента (модели). Это увеличивает шансы того, что модель будет генерировать корректный ответ на запрос вместо, например, продолжения введенного пользователем текста ","metadata":{}},{"id":"8ab24edd","cell_type":"markdown","source":"## Походы в API - 10 баллов\n\nТеперь давайте посмотрим, как можно ходить в API. Для примера мы будем ходить в together.ai, который щедро предоставляет $5 всем зарегистрировавшимся. Вообще говоря различных провайдеров много, API у них у всех очень похожий, т.к. все мимикрируют под OpenAI.","metadata":{"id":"8ab24edd"}},{"id":"a3f3931a","cell_type":"code","source":"# Вставьте свой ключ из https://api.together.ai/\nAPI_KEY = \"8be9b43c22a28abd4152c6957710d54cec55fc938a0db0fc236fa7b4ead23f59\"","metadata":{"id":"a3f3931a","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:28.234282Z","iopub.execute_input":"2025-02-10T16:28:28.234602Z","iopub.status.idle":"2025-02-10T16:28:28.238629Z","shell.execute_reply.started":"2025-02-10T16:28:28.234578Z","shell.execute_reply":"2025-02-10T16:28:28.237565Z"}},"outputs":[],"execution_count":14},{"id":"d5df9d33","cell_type":"markdown","source":"Есть несколько способов сходить в API. Можно ходить напрямую через библиотеку **requests**. Допишите post запрос в `url` с данными `data` и заголовками `headers`.","metadata":{"id":"d5df9d33"}},{"id":"9e9f97ed","cell_type":"code","source":"headers = {\n    'Authorization': 'Bearer ' + API_KEY,\n    'Content-Type': 'application/json',\n}\nurl = \"https://api.together.xyz/v1/chat/completions\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n]\n\ndata = {\n    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n    \"messages\": messages\n}\n\nresponse = requests.post(url=url, headers=headers, json=data) # Ваш код здесь\nmodel_answer = response.json()['choices'][0]['message']['content'] # Ваш код здесь\nassert \"london\" in model_answer.lower()","metadata":{"id":"9e9f97ed","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:29.706904Z","iopub.execute_input":"2025-02-10T16:28:29.707257Z","iopub.status.idle":"2025-02-10T16:28:29.994650Z","shell.execute_reply.started":"2025-02-10T16:28:29.707227Z","shell.execute_reply":"2025-02-10T16:28:29.993771Z"}},"outputs":[],"execution_count":15},{"id":"d3fb9bcc","cell_type":"code","source":"print(model_answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:33.799690Z","iopub.execute_input":"2025-02-10T16:28:33.800239Z","iopub.status.idle":"2025-02-10T16:28:33.805428Z","shell.execute_reply.started":"2025-02-10T16:28:33.800200Z","shell.execute_reply":"2025-02-10T16:28:33.804275Z"}},"outputs":[{"name":"stdout","text":"The capital of the United Kingdom (Britain) is London.\n","output_type":"stream"}],"execution_count":16},{"id":"a8efb521","cell_type":"markdown","source":"Мы подали messages, дальше они каким-то образом собрались в promt и подались модели. Мы не знаем, какой промпт используется на стороне провайдера. Вспомним про Today Date из предыдущего пункта задания - использует ли его together? Обновляют ли они его сегодняшним днем или оставляют 26 июля? Если обновляют, то по какому часовому поясу?\n\nЧтобы ответы на эти и многие другие вопросы не мучали нас по ночам, можно использовать prompt формат, а именно подать модели текст напрямую на генерацию. Давайте для этого используем `tokenizer.apply_chat_template`. Модель будет принимать текст ровно так, как вы его подадите, без каких-либо предобработок. Подумайте, нужно ли вам использовать аргумент `add_generation_prompt`?\n\nЧтобы послать запрос напрямую, нужно в предыдущем запросе убрать messages, который представляет из себя список словарей, и послать поле prompt - строку с промптом для модели.","metadata":{"id":"a8efb521"}},{"id":"772bd506","cell_type":"code","source":"headers = {\n    'Authorization': 'Bearer ' + API_KEY,\n    'Content-Type': 'application/json',\n}\nurl = \"https://api.together.xyz/v1/chat/completions\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n]\n\nprompt = tokenizer.apply_chat_template(chat_template=tokenizer.chat_template, conversation=messages, tokenize=False, add_generation_prompt=True)\n\ndata = {\n    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n    \"prompt\": prompt # Ваш код здесь\n}\n\nresponse = requests.post(url=url, headers=headers, json=data) # Ваш код здесь\nmodel_answer = response.json()['choices'][0]['message']['content'] # Ваш код здесь\nassert \"london\" in model_answer.lower() and \"assistant\" not in model_answer.lower()","metadata":{"id":"772bd506","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:36.006316Z","iopub.execute_input":"2025-02-10T16:28:36.006674Z","iopub.status.idle":"2025-02-10T16:28:36.271198Z","shell.execute_reply.started":"2025-02-10T16:28:36.006640Z","shell.execute_reply":"2025-02-10T16:28:36.270121Z"}},"outputs":[],"execution_count":17},{"id":"18e4b401","cell_type":"code","source":"print(model_answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:38.581441Z","iopub.execute_input":"2025-02-10T16:28:38.581802Z","iopub.status.idle":"2025-02-10T16:28:38.586591Z","shell.execute_reply.started":"2025-02-10T16:28:38.581772Z","shell.execute_reply":"2025-02-10T16:28:38.585474Z"}},"outputs":[{"name":"stdout","text":"The capital of Britain, also known as the United Kingdom (UK), is London.\n","output_type":"stream"}],"execution_count":18},{"id":"9789bff9","cell_type":"markdown","source":"## Клиент - 5 баллов\n\nТеперь мы понимаем общую схему взаимодействия с провайдером - они предоставляют апи, куда можно посылать или промтп или историю диалога. При посылке промпта вся ответственность за формат ложится на нас, при посылке messages форматтинг происходит на стороне провайдера, но мы не всегда представляем, как он работает. Выбор в пользу того или иного варианта всегда остается на вас.\n\nМы использовали выше библиотеку requests, чтобы послать HTTP-запрос на сервера together, однако есть способ и проще - python client. Давайте познакомимся с ним поближе. Для этого давайте используем функцию `client.chat.completions.create`. Также давайте добавим опции сэмплинга, которые в этой функции поддержаны. Их можно посылать и в запросах через requests, но мы здесь и далее будем пользоваться клиентом.\n* top_k = 100\n* temperature = 0.5\n* top_p = 0.9\n* repetition_penalty = 1.05","metadata":{"id":"9789bff9"}},{"id":"88dc9341","cell_type":"code","source":"client = Together(api_key=API_KEY)","metadata":{"id":"88dc9341","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:41.506728Z","iopub.execute_input":"2025-02-10T16:28:41.507135Z","iopub.status.idle":"2025-02-10T16:28:41.511285Z","shell.execute_reply.started":"2025-02-10T16:28:41.507092Z","shell.execute_reply":"2025-02-10T16:28:41.510280Z"}},"outputs":[],"execution_count":19},{"id":"b88e3ef8","cell_type":"code","source":"model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of Britain?\"}\n]","metadata":{"id":"b88e3ef8","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:42.794722Z","iopub.execute_input":"2025-02-10T16:28:42.795125Z","iopub.status.idle":"2025-02-10T16:28:42.799352Z","shell.execute_reply.started":"2025-02-10T16:28:42.795095Z","shell.execute_reply":"2025-02-10T16:28:42.798309Z"}},"outputs":[],"execution_count":20},{"id":"9f88e0c8","cell_type":"code","source":"response = client.chat.completions.create(\n    model=model_name,\n    messages=messages,\n    top_k=100,\n    top_p=0.9,\n    temperature=0.5,\n    repetition_penalty=1.05\n)\n\nresponse_text = response.choices[0].message.content # Ваш код здесь\nassert \"london\" in response_text.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:45.218554Z","iopub.execute_input":"2025-02-10T16:28:45.219111Z","iopub.status.idle":"2025-02-10T16:28:45.771458Z","shell.execute_reply.started":"2025-02-10T16:28:45.219064Z","shell.execute_reply":"2025-02-10T16:28:45.770646Z"}},"outputs":[],"execution_count":21},{"id":"1c0c4a4c","cell_type":"markdown","source":"Аналогично посылать просто prompt можно через `client.completions.create`.","metadata":{"id":"1c0c4a4c"}},{"id":"08d5758f","cell_type":"markdown","source":"## Tools - 5 баллов\n\nДавайте теперь посмотрим, как можно использовать tools в связке с моделями. У нас есть функция, которая входит в базу данных и получает информацию о юзере. Базы данных, конечно же, у нас никакой нет, но у нас есть некоторая функция, которая эмулирует это поведение, так что давайте попробуем ее описать.\n","metadata":{"id":"08d5758f"}},{"id":"2dd5090a","cell_type":"code","source":"def get_user_info_from_db(person_name: str) -> Dict[str, str]:\n    database = {\n        \"ilya\": {\n            \"job\": \"Software Developer\",\n            \"pets\": \"dog\",\n        },\n        \"farruh\": {\n            \"job\": \"Senior Data & Solution Architect\",\n            \"hobby\": \"travelling, hiking\",\n        },\n        \"timur\": {\n            \"job\": \"DeepSchool Founder\",\n            \"city\": \"Novosibirsk\",\n        }\n    }\n    no_info = {\"err\": f\"No info about {person_name}\"}\n    return database.get(person_name.lower(), no_info)\n\nprint(get_user_info_from_db(\"Timur\"))","metadata":{"id":"2dd5090a","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:49.491328Z","iopub.execute_input":"2025-02-10T16:28:49.491637Z","iopub.status.idle":"2025-02-10T16:28:49.497770Z","shell.execute_reply.started":"2025-02-10T16:28:49.491614Z","shell.execute_reply":"2025-02-10T16:28:49.496899Z"}},"outputs":[{"name":"stdout","text":"{'job': 'DeepSchool Founder', 'city': 'Novosibirsk'}\n","output_type":"stream"}],"execution_count":22},{"id":"bf7e8052","cell_type":"markdown","source":"Давайте попробуем описать эту функцию в формате json, чтобы модель могла ее увидеть!\nЗаполните поля в определении дальше","metadata":{"id":"bf7e8052"}},{"id":"dbb3a3cf","cell_type":"code","source":"get_user_info_from_db_tool = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_user_info_from_db\",\n        \"description\": \"Function to get dictionary with user info from database\", # Напишите, что функция делает своими словами\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"person_name\": {\n                    \"type\": \"string\",\n                    \"description\": \"First name of a person\" # Опишите смысл аргумента\n                }\n            },\n            \"required\": [\"person_name\"] # укажите обязательные аргументы для функции\n        }\n    }\n}","metadata":{"id":"dbb3a3cf","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:51.760113Z","iopub.execute_input":"2025-02-10T16:28:51.760446Z","iopub.status.idle":"2025-02-10T16:28:51.765421Z","shell.execute_reply.started":"2025-02-10T16:28:51.760421Z","shell.execute_reply":"2025-02-10T16:28:51.764306Z"}},"outputs":[],"execution_count":23},{"id":"5e9f5bd6","cell_type":"markdown","source":"Теперь давайте подадим это описание в `tokenizer.apply_chat_template`. Обратите внимание на его аргумент `tools`! Не забудьте `add_generation_prompt`, если он нужен.","metadata":{"id":"5e9f5bd6"}},{"id":"0ad9aaf2","cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n]\nprompt = tokenizer.apply_chat_template(conversation=messages, tools=[get_user_info_from_db_tool], tokenize=False, add_generation_prompt=True) # Ваш код здесь\nprint(prompt)","metadata":{"id":"0ad9aaf2","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:54.008478Z","iopub.execute_input":"2025-02-10T16:28:54.008875Z","iopub.status.idle":"2025-02-10T16:28:54.015056Z","shell.execute_reply.started":"2025-02-10T16:28:54.008822Z","shell.execute_reply":"2025-02-10T16:28:54.013920Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nEnvironment: ipython\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n\nRespond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables.\n\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_user_info_from_db\",\n        \"description\": \"Function to get dictionary with user info from database\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"person_name\": {\n                    \"type\": \"string\",\n                    \"description\": \"First name of a person\"\n                }\n            },\n            \"required\": [\n                \"person_name\"\n            ]\n        }\n    }\n}\n\nWhat do you know about Ilya?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n","output_type":"stream"}],"execution_count":24},{"id":"e980a9be","cell_type":"markdown","source":"Давайте пошлем наш запрос в модель. На выбор 2 модели, если не будет работать с 8b, то предлагается посылать в 70b.\nДля данного запроса для 8b был подобран работающий `seed=9706540181089681000`, который можно подать в функцию.\n\nДавайте воспользуемся `client.completions.create` для генерации ответа от модели.","metadata":{"id":"e980a9be"}},{"id":"25e080b5","cell_type":"code","source":"model_8b = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\"\nmodel_70b = \"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\"","metadata":{"id":"25e080b5","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:56.775809Z","iopub.execute_input":"2025-02-10T16:28:56.776176Z","iopub.status.idle":"2025-02-10T16:28:56.780556Z","shell.execute_reply.started":"2025-02-10T16:28:56.776148Z","shell.execute_reply":"2025-02-10T16:28:56.779419Z"}},"outputs":[],"execution_count":25},{"id":"b2cdaebf","cell_type":"code","source":"response_8b = client.completions.create(model=model_8b, prompt=prompt, seed=9706540181089681000) # Ваш код здесь\nresponse_70b = client.completions.create(model=model_70b, prompt=prompt, seed=9706540181089681000) # Ваш код здесь\n\nprint(response_8b.choices[0].text)\nprint(response_70b.choices[0].text)","metadata":{"id":"b2cdaebf","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:28:58.642611Z","iopub.execute_input":"2025-02-10T16:28:58.643014Z","iopub.status.idle":"2025-02-10T16:29:09.125827Z","shell.execute_reply.started":"2025-02-10T16:28:58.642982Z","shell.execute_reply":"2025-02-10T16:29:09.124828Z"}},"outputs":[{"name":"stdout","text":"{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\n{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\n","output_type":"stream"}],"execution_count":26},{"id":"3b870a67","cell_type":"markdown","source":"Если все хорошо, то мы получили ответ от модели, который выглядит как некоторый структурированный вывод, который можно использовать для вызова модели. Давайте попробуем написать функцию, которая принимает ответ модели в \"сыром виде\", выбирает, какую функцию с какими аргументами вызвать.\n\nЗдесь нам поможет FUNCTION_REGISTRY и то, что параметры в функцию можно передавать как словарь, например так\n```python\ndef foo(a, b, c):\n    print(a, b, c)\n\nobj = {'b':10, 'c':'lee'}\n\nfoo(100, **obj)\n```","metadata":{"id":"3b870a67"}},{"id":"92feaf07","cell_type":"code","source":"FUNCTION_REGISTRY = {\"get_user_info_from_db\": get_user_info_from_db}\n# На случай, если модель не генерит function call\nreference_answer = \"\"\"{\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}\"\"\"","metadata":{"id":"92feaf07","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:29:11.091766Z","iopub.execute_input":"2025-02-10T16:29:11.092160Z","iopub.status.idle":"2025-02-10T16:29:11.097043Z","shell.execute_reply.started":"2025-02-10T16:29:11.092119Z","shell.execute_reply":"2025-02-10T16:29:11.095978Z"}},"outputs":[],"execution_count":27},{"id":"00f5515b","cell_type":"code","source":"def parse_function_call(model_answer):\n    # Ваш код здесь\n    # 1. Проверим, является ли это function call.\n    dict_answer = json.loads(model_answer)\n    function_call = dict_answer['name']\n    if function_call not in FUNCTION_REGISTRY:\n        return f\"Error: {function_call} is not a function call\"\n    # 2. Вызов нужной функции с указанными аргументами\n    return FUNCTION_REGISTRY[function_call](**dict_answer['parameters'])\n\n\nassert parse_function_call(reference_answer) == get_user_info_from_db(\"Ilya\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:29:13.022339Z","iopub.execute_input":"2025-02-10T16:29:13.022650Z","iopub.status.idle":"2025-02-10T16:29:13.027600Z","shell.execute_reply.started":"2025-02-10T16:29:13.022627Z","shell.execute_reply":"2025-02-10T16:29:13.026561Z"}},"outputs":[],"execution_count":28},{"id":"eed947f2","cell_type":"markdown","source":"Теперь давайте попробуем объединить все это в историю диалога и сгенерировать моделью финальный ответ.\nДля этого в messages, где хранится наша история диалога нужно добавить\n1. Вызов function call моделью с ролью ХХХ (это часть задания, напишите сами)\n2. Ответ function call с ролью tool\n\nПосле этого данный промпт нужно послать модели снова, чтобы получить финальный ответ.\nДля этого опять используем `tokenizer.apply_chat_template` и `client.completions.create`.\n\nВ зависимости от модели может понадобиться убрать tools (на 8b, 70b должна справиться). Для 8b опять же подобран seed=2017684582943914000","metadata":{"id":"eed947f2"}},{"id":"5e7e6d47","cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n]\n# Добавляем ответ модели\nmodel_answer = json.dumps({\n    \"name\": \"get_user_info_from_db\", \n    \"parameters\": {\"person_name\": \"Ilya\"}\n})\nmessages.append(\n    {\"role\": \"assistant\", \"content\": f\"Calling function: {model_answer}\"}\n)\n# Добавляем ответ tool\ntool_answer = parse_function_call(model_answer)\nmessages.append(\n    {\"role\": \"tool\", \"name\": \"get_user_info_from_db\", \"content\": json.dumps(tool_answer)}\n)\n\nprompt = tokenizer.apply_chat_template(conversation=messages, tokenize=False, add_generation_prompt=True)\nprint(prompt)","metadata":{"id":"5e7e6d47","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:29:16.157473Z","iopub.execute_input":"2025-02-10T16:29:16.157785Z","iopub.status.idle":"2025-02-10T16:29:16.164758Z","shell.execute_reply.started":"2025-02-10T16:29:16.157761Z","shell.execute_reply":"2025-02-10T16:29:16.163883Z"}},"outputs":[{"name":"stdout","text":"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat do you know about Ilya?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nCalling function: {\"name\": \"get_user_info_from_db\", \"parameters\": {\"person_name\": \"Ilya\"}}<|eot_id|><|start_header_id|>ipython<|end_header_id|>\n\n\"{\\\"job\\\": \\\"Software Developer\\\", \\\"pets\\\": \\\"dog\\\"}\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n","output_type":"stream"}],"execution_count":29},{"id":"26d12590","cell_type":"code","source":"response_8b = client.completions.create(model=model_70b, prompt=prompt, seed=2017684582943914000)\nprint(response_8b.choices[0].text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:29:20.649336Z","iopub.execute_input":"2025-02-10T16:29:20.649659Z","iopub.status.idle":"2025-02-10T16:29:24.290015Z","shell.execute_reply.started":"2025-02-10T16:29:20.649633Z","shell.execute_reply":"2025-02-10T16:29:24.289138Z"}},"outputs":[{"name":"stdout","text":"Ilya is a software developer and has a dog.\n","output_type":"stream"}],"execution_count":30},{"id":"93ce7746","cell_type":"markdown","source":"Теперь давайте посмотрим на chat-API, как обрабатываются function calls там?\nИспользуем для этого уже знакомый `client.chat.completions.create`, обратим внимание на аргумент tools внутри него. Здесь рекомендуется использовать 70b модель. На всякий случай работающий seed=14157400267283583000","metadata":{"id":"93ce7746"}},{"id":"bae78906","cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n]\nresponse = client.chat.completions.create(model=model_70b, messages=messages, tools=[get_user_info_from_db_tool], seed=14157400267283583000) # Ваш код здесь","metadata":{"id":"bae78906","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:29:25.214286Z","iopub.execute_input":"2025-02-10T16:29:25.214599Z","iopub.status.idle":"2025-02-10T16:29:28.635596Z","shell.execute_reply.started":"2025-02-10T16:29:25.214576Z","shell.execute_reply":"2025-02-10T16:29:28.634772Z"}},"outputs":[],"execution_count":31},{"id":"f9e2dd36","cell_type":"markdown","source":"Мы можем видеть, что у нас не работает предыдущий подход с полем `content`, однако должно было появиться поле `tool_calls`, которое содержит в себе информацию о вызове инструмента","metadata":{"id":"f9e2dd36"}},{"id":"13c84279","cell_type":"code","source":"response.choices[0].message.tool_calls","metadata":{"id":"13c84279","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:29:28.860564Z","iopub.execute_input":"2025-02-10T16:29:28.860949Z","iopub.status.idle":"2025-02-10T16:29:28.866798Z","shell.execute_reply.started":"2025-02-10T16:29:28.860919Z","shell.execute_reply":"2025-02-10T16:29:28.865934Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[ToolCalls(id='call_1jbpoaa0eo940h3kyw9tahcq', type='function', function=FunctionCall(name='get_user_info_from_db', arguments='{\"person_name\":\"Ilya\"}'), index=0)]"},"metadata":{}}],"execution_count":32},{"id":"e907c917","cell_type":"markdown","source":"# Использование библиотек\n\nТеперь, когда мы руками прошли весь пути обработки function call можно посмотреть уже на готовые инструменты.\nМы много чего сделали руками:\n1. Писали описание функции\n2. Обрабатывали ответ\n3. Вызывали функцию\n4. Возвращали все это в модель\n\nДавайте теперь посмотрим, как оно работает в библиотеках!\n\n**NB** - библиотеки развиваются и вполне, возможно, что к концу курса те интерфейсы, которые мы используем в этом домашнем задании будут уже неактуальны, но я уверен, что знаний и принципов, полученных из этих заданий хватит, чтобы адаптироваться к будущим вызовам!","metadata":{"id":"e907c917"}},{"id":"6d89c627","cell_type":"code","source":"! pip install langchain==0.2.16 llama-index-core==v0.11.16 langchain-together llama-index-llms-together==0.2.0","metadata":{"id":"6d89c627","scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"id":"51b964f9","cell_type":"markdown","source":"# LangChain - 5 баллов","metadata":{"id":"51b964f9"}},{"id":"5ef83e32","cell_type":"code","source":"import os\nfrom langchain_together import ChatTogether\nfrom langchain_core.tools import tool","metadata":{"id":"5ef83e32","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:29:50.870160Z","iopub.execute_input":"2025-02-10T16:29:50.870507Z","iopub.status.idle":"2025-02-10T16:29:52.758568Z","shell.execute_reply.started":"2025-02-10T16:29:50.870477Z","shell.execute_reply":"2025-02-10T16:29:52.757446Z"}},"outputs":[],"execution_count":34},{"id":"0a7b08ae","cell_type":"markdown","source":"Давайте ознакомимся с langchain-интеграцией together.ai","metadata":{"id":"0a7b08ae"}},{"id":"f7f75658","cell_type":"code","source":"os.environ[\"TOGETHER_API_KEY\"] = API_KEY\n\nllm = ChatTogether(\n    model=model_70b\n)","metadata":{"id":"f7f75658","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:29:56.591614Z","iopub.execute_input":"2025-02-10T16:29:56.591987Z","iopub.status.idle":"2025-02-10T16:29:56.719275Z","shell.execute_reply.started":"2025-02-10T16:29:56.591955Z","shell.execute_reply":"2025-02-10T16:29:56.718312Z"}},"outputs":[],"execution_count":36},{"id":"9d548c4e","cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"What do you know about Ilya?\"}\n]\nresponse = llm.invoke(messages)\nprint(response.content)","metadata":{"id":"9d548c4e","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:29:58.472461Z","iopub.execute_input":"2025-02-10T16:29:58.472787Z","iopub.status.idle":"2025-02-10T16:30:02.510582Z","shell.execute_reply.started":"2025-02-10T16:29:58.472762Z","shell.execute_reply":"2025-02-10T16:30:02.509521Z"}},"outputs":[{"name":"stdout","text":"Ilya can refer to several things, but I'll provide a few possibilities:\n\n1. **Ilya Kovalchuk**: Ilya Valeryevich Kovalchuk is a Russian professional ice hockey player who currently plays for Avtomobilist Yekaterinburg of the Kontinental Hockey League (KHL). He's a highly skilled winger and has played in the NHL for teams like the Atlanta Thrashers, New Jersey Devils, and Los Angeles Kings.\n\n2. **Ilya Repin**: Ilya Yefimovich Repin was a Russian painter and artist who lived from 1844 to 1930. He's known for his realistic and detailed depictions of everyday life, as well as his portraits of famous Russians, such as Leo Tolstoy and Modest Mussorgsky.\n\n3. **Ilya Muromets**: Ilya Muromets is a legendary hero from medieval Russian folklore. He's often depicted as a strong and brave warrior who protects the people and fights against evil forces.\n\n4. **Ilya (musician)**: Ilya is also a British electronic music producer and DJ. His real name is Ilya Salmanzadeh, and he's known for his work with artists like Ariana Grande, Taylor Swift, and Katy Perry.\n\nIf you could provide more context or information about the Ilya you're referring to, I'll be happy to try and provide more specific information.\n","output_type":"stream"}],"execution_count":37},{"id":"1d698905","cell_type":"markdown","source":"Теперь, когда мы разобрались, как базово работать с langchain, давайте попробуем добавить инструментов. Чтобы нам было не так скучно, давайте напишем новую функцию, которая считает \"волшебную операцию\".\n\nЭта функция принимает 2 строки, возвращает строку строку b в обратном порядке, сконкатенированную со строкой a. Допишите эту функцию.","metadata":{"id":"1d698905"}},{"id":"bb6976bf","cell_type":"code","source":"def magic_operation(a, b):\n    return b[::-1] + a\n\nassert magic_operation(\"456\", \"321\") == \"123456\"","metadata":{"id":"bb6976bf","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:30:06.495053Z","iopub.execute_input":"2025-02-10T16:30:06.495418Z","iopub.status.idle":"2025-02-10T16:30:06.499876Z","shell.execute_reply.started":"2025-02-10T16:30:06.495386Z","shell.execute_reply":"2025-02-10T16:30:06.498910Z"}},"outputs":[],"execution_count":38},{"id":"89726ec1","cell_type":"markdown","source":"Теперь давайте обернем эту функцию в декоратор tool из langchain, аннотируем типы и допишем docstring. После этого можно будет автоматически сгенерировать описани функции в function call формате!","metadata":{"id":"89726ec1"}},{"id":"482c8b38","cell_type":"code","source":"@tool\ndef magic_operation_tool(a: str, b: str) -> str:\n    \"\"\"Reverse b and concatenate a\"\"\"\n    return magic_operation(a, b)\n\nprint(magic_operation_tool.args_schema.schema())","metadata":{"id":"482c8b38","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:30:09.025173Z","iopub.execute_input":"2025-02-10T16:30:09.025524Z","iopub.status.idle":"2025-02-10T16:30:09.036751Z","shell.execute_reply.started":"2025-02-10T16:30:09.025495Z","shell.execute_reply":"2025-02-10T16:30:09.035781Z"}},"outputs":[{"name":"stdout","text":"{'title': 'magic_operation_toolSchema', 'description': 'Reverse b and concatenate a', 'type': 'object', 'properties': {'a': {'title': 'A', 'type': 'string'}, 'b': {'title': 'B', 'type': 'string'}}, 'required': ['a', 'b']}\n","output_type":"stream"}],"execution_count":39},{"id":"5a60cf8b","cell_type":"markdown","source":"Теперь давайте попробуем подать запрос в нашу LLM и обогатить ее нашим function_call. Для этого нужна функция `llm.bind_tools`.","metadata":{"id":"5a60cf8b"}},{"id":"f6c465f4","cell_type":"code","source":"llm_with_tools = llm.bind_tools(tools=[magic_operation_tool])","metadata":{"id":"f6c465f4","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:30:11.698908Z","iopub.execute_input":"2025-02-10T16:30:11.699285Z","iopub.status.idle":"2025-02-10T16:30:11.863487Z","shell.execute_reply.started":"2025-02-10T16:30:11.699253Z","shell.execute_reply":"2025-02-10T16:30:11.862464Z"}},"outputs":[],"execution_count":40},{"id":"d0db362d","cell_type":"markdown","source":"Теперь давайте как и раньше:\n1. Сгенерируем ответ на messages\n2. Проверим в ответе resp.tool_calls, вызовем нужный инструмент\n3. Расширим messages ответом модели и ответом инструмента, сгенерируем финальный ответ.","metadata":{"id":"d0db362d"}},{"id":"b15a3c3f","cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n    {\"role\": \"user\", \"content\": \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"}\n]\nresp = llm_with_tools.invoke(messages)","metadata":{"id":"b15a3c3f","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:37:59.756444Z","iopub.execute_input":"2025-02-10T16:37:59.756769Z","iopub.status.idle":"2025-02-10T16:38:00.230717Z","shell.execute_reply.started":"2025-02-10T16:37:59.756745Z","shell.execute_reply":"2025-02-10T16:38:00.229659Z"}},"outputs":[],"execution_count":76},{"id":"e3f86c0e","cell_type":"code","source":"print(resp.tool_calls[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:01.511636Z","iopub.execute_input":"2025-02-10T16:38:01.512045Z","iopub.status.idle":"2025-02-10T16:38:01.517189Z","shell.execute_reply.started":"2025-02-10T16:38:01.512004Z","shell.execute_reply":"2025-02-10T16:38:01.516028Z"}},"outputs":[{"name":"stdout","text":"{'name': 'magic_operation_tool', 'args': {'a': '456', 'b': '321'}, 'id': 'call_4qtxcf8txdv9uk8z77bywbhr', 'type': 'tool_call'}\n","output_type":"stream"}],"execution_count":77},{"id":"99d72f80","cell_type":"code","source":"messages.append(\n    {\"role\": \"assistant\", \"content\": f\"Output: function call {resp.tool_calls[0]}\"}\n)\nmessages.append(\n    {\"role\": \"tool\", \"tool_call_id\": resp.tool_calls[0][\"id\"], \"content\": magic_operation(**resp.tool_calls[0][\"args\"])}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:03.111892Z","iopub.execute_input":"2025-02-10T16:38:03.112252Z","iopub.status.idle":"2025-02-10T16:38:03.117037Z","shell.execute_reply.started":"2025-02-10T16:38:03.112222Z","shell.execute_reply":"2025-02-10T16:38:03.116065Z"}},"outputs":[],"execution_count":78},{"id":"d326c1e4","cell_type":"code","source":"messages","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:05.137411Z","iopub.execute_input":"2025-02-10T16:38:05.137766Z","iopub.status.idle":"2025-02-10T16:38:05.143725Z","shell.execute_reply.started":"2025-02-10T16:38:05.137735Z","shell.execute_reply":"2025-02-10T16:38:05.142704Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"[{'role': 'system', 'content': 'You are a helpful assistant'},\n {'role': 'user',\n  'content': 'Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`'},\n {'role': 'assistant',\n  'content': \"Output: function call {'name': 'magic_operation_tool', 'args': {'a': '456', 'b': '321'}, 'id': 'call_4qtxcf8txdv9uk8z77bywbhr', 'type': 'tool_call'}\"},\n {'role': 'tool',\n  'tool_call_id': 'call_4qtxcf8txdv9uk8z77bywbhr',\n  'content': '123456'}]"},"metadata":{}}],"execution_count":79},{"id":"f9a20e11","cell_type":"code","source":"assert len(messages) == 4","metadata":{"id":"f9a20e11","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:07.612242Z","iopub.execute_input":"2025-02-10T16:38:07.612609Z","iopub.status.idle":"2025-02-10T16:38:07.616919Z","shell.execute_reply.started":"2025-02-10T16:38:07.612578Z","shell.execute_reply":"2025-02-10T16:38:07.615752Z"}},"outputs":[],"execution_count":80},{"id":"0d462fa5","cell_type":"code","source":"res = llm.invoke(messages).content\nassert \"123456\" in res","metadata":{"id":"0d462fa5","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:09.236415Z","iopub.execute_input":"2025-02-10T16:38:09.236755Z","iopub.status.idle":"2025-02-10T16:38:09.575094Z","shell.execute_reply.started":"2025-02-10T16:38:09.236730Z","shell.execute_reply":"2025-02-10T16:38:09.574328Z"}},"outputs":[],"execution_count":81},{"id":"3feb1a5b","cell_type":"markdown","source":"# LlamaIndex - 5 баллов\n\nАналогичный инструмент LlamaIndex. В ней не так хороша поддержка function calls не для OpenAI, поэтому придется забежать вперед и использовать ReActAgent.","metadata":{"id":"3feb1a5b"}},{"id":"9e6485db","cell_type":"code","source":"from llama_index.llms.together import TogetherLLM\nfrom llama_index.core.llms import ChatMessage\nfrom llama_index.core.tools import FunctionTool\nfrom llama_index.core.agent import ReActAgent","metadata":{"id":"9e6485db","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:12.273573Z","iopub.execute_input":"2025-02-10T16:38:12.274080Z","iopub.status.idle":"2025-02-10T16:38:14.927879Z","shell.execute_reply.started":"2025-02-10T16:38:12.274035Z","shell.execute_reply":"2025-02-10T16:38:14.927098Z"}},"outputs":[],"execution_count":82},{"id":"cfef1b75","cell_type":"code","source":"llm = TogetherLLM(model=model_70b, api_key=API_KEY)","metadata":{"id":"cfef1b75","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:15.916225Z","iopub.execute_input":"2025-02-10T16:38:15.916957Z","iopub.status.idle":"2025-02-10T16:38:15.921578Z","shell.execute_reply.started":"2025-02-10T16:38:15.916923Z","shell.execute_reply":"2025-02-10T16:38:15.920473Z"}},"outputs":[],"execution_count":83},{"id":"b6ea9b3e","cell_type":"markdown","source":"Скопируйте magic_operation_tool из части с langchain сюда,  но без декоратора.","metadata":{"id":"b6ea9b3e"}},{"id":"4b6a47a7","cell_type":"code","source":"def magic_operation_tool(a: str, b: str) -> str:\n    \"\"\"Reverse b and concatenate a\"\"\"\n    print(\"INSIDE FUNCTION CALL\")\n    return magic_operation(a, b)","metadata":{"id":"4b6a47a7","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:18.314405Z","iopub.execute_input":"2025-02-10T16:38:18.314748Z","iopub.status.idle":"2025-02-10T16:38:18.319092Z","shell.execute_reply.started":"2025-02-10T16:38:18.314721Z","shell.execute_reply":"2025-02-10T16:38:18.318088Z"}},"outputs":[],"execution_count":84},{"id":"3bfc72e3","cell_type":"markdown","source":"Мы можем аналогично создать инструмент с помощью `FunctionTool.from_defaults`","metadata":{"id":"3bfc72e3"}},{"id":"b50ff121","cell_type":"code","source":"magic_operation_tool_llamaindex = FunctionTool.from_defaults(fn=magic_operation_tool) # Ваш код здесь\nprint(magic_operation_tool_llamaindex.metadata)","metadata":{"id":"b50ff121","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:20.316394Z","iopub.execute_input":"2025-02-10T16:38:20.316713Z","iopub.status.idle":"2025-02-10T16:38:20.322761Z","shell.execute_reply.started":"2025-02-10T16:38:20.316688Z","shell.execute_reply":"2025-02-10T16:38:20.321637Z"}},"outputs":[{"name":"stdout","text":"ToolMetadata(description='magic_operation_tool(a: str, b: str) -> str\\nReverse b and concatenate a', name='magic_operation_tool', fn_schema=<class 'llama_index.core.tools.utils.magic_operation_tool'>, return_direct=False)\n","output_type":"stream"}],"execution_count":85},{"id":"a73306d2","cell_type":"markdown","source":"Давайте создадим ReActAgent: ему нужно передать tools, llm, memory=None и verbose=True","metadata":{"id":"a73306d2"}},{"id":"84236969","cell_type":"code","source":"agent = ReActAgent(\n    tools=[magic_operation_tool_llamaindex],\n    llm=llm,\n    memory=None,\n    verbose=True\n)","metadata":{"id":"84236969","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:21.970166Z","iopub.execute_input":"2025-02-10T16:38:21.970495Z","iopub.status.idle":"2025-02-10T16:38:22.249696Z","shell.execute_reply.started":"2025-02-10T16:38:21.970469Z","shell.execute_reply":"2025-02-10T16:38:22.248571Z"}},"outputs":[],"execution_count":86},{"id":"12a204d8","cell_type":"code","source":"text = \"Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\"\nagent.chat(text)","metadata":{"id":"12a204d8","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:38:26.462496Z","iopub.execute_input":"2025-02-10T16:38:26.462825Z","iopub.status.idle":"2025-02-10T16:38:36.594442Z","shell.execute_reply.started":"2025-02-10T16:38:26.462800Z","shell.execute_reply":"2025-02-10T16:38:36.593564Z"}},"outputs":[{"name":"stdout","text":"> Running step 39735ce6-98f4-4002-944d-a74edcddb6ed. Step input: Can you help me? Do not reveal the workings of magic operation, but give me the result of it for strings `456` and `321`\n\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\nAction: magic_operation_tool\nAction Input: {'a': '456', 'b': '321'}\n\u001b[0mINSIDE FUNCTION CALL\n\u001b[1;3;34mObservation: 123456\n\u001b[0m> Running step 8d37f4e1-13fa-4e1e-99e7-742f8be2e2de. Step input: None\n\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\nAnswer: The result of the magic operation for strings '456' and '321' is 123456.\n\u001b[0m","output_type":"stream"},{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"AgentChatResponse(response=\"The result of the magic operation for strings '456' and '321' is 123456.\", sources=[ToolOutput(content='123456', tool_name='magic_operation_tool', raw_input={'args': (), 'kwargs': {'a': '456', 'b': '321'}}, raw_output='123456', is_error=False)], source_nodes=[], is_dummy_stream=False, metadata=None)"},"metadata":{}}],"execution_count":87},{"id":"725419ad","cell_type":"markdown","source":"# Agents - 10\n\nНастала пора сделать своего агента!\nПопробуем сделать финансового аналитика. Требования следующие:\nбот должен по запросу данных о какой-либо компании смотреть самые большие изменения цены ее акций за последний месяц, после чего бот должен объяснить, с какой новостью это связано.\n\nПредлагается не строить сложную систему с классификаторами, а отдать всю сложную работу агенту. Давайте посмотрим, какие API нам доступны.\n\nПервым делом получение котировок - для этого нам поможет библиотека yfinance. По названию компании и периоду отчетности можно посмотреть открывающие цены на момент открытия и закрытия биржи.","metadata":{"id":"725419ad"}},{"id":"32f75126","cell_type":"markdown","source":"Очень много статей заблокированы и имеют название `[Removed]`, нужно их отфильтровать. В оставшихся статьях будем брать только title (заголовок) и description (описание или краткий пересказ).","metadata":{"id":"32f75126"}},{"id":"cc9378f6","cell_type":"markdown","source":"Вам необходимо реализовать [ReAct Agent](https://react-lm.github.io/). Особенность этого агента заключается в том, что он вначале формирует мысль, а потом вызывает действие (function call) для достижения какой-либо цели.\n\nЧто нужно сделать:\n1. Описать и реализовать function call для определения, в какой день была самая большая разница в цене акций в момент открытия и закрытия биржи. Функция получает один аргумент - название акций компании (например AAPL для Apple), а выдает словарь с 2мя полями: с датой максимальной разницы в ценах и самой разницей в ценах.\n2. Описать и реализовать function call для получения 5 релевантных новостей о компании. В качестве аргумента принимаются название компании и дата. Ваша задача - сходить в newsapi, получить новости и вернуть 5 случайных новостей, которые произошли не позже чем день торгов. Если новостей меньше 5, то верните столько, сколько получится.\n3. После этого агент должен вернуть ответ, в котором постарается аргументировать изменения в цене.\n\n\nРеализовывать агента можно любым удобным способом, в том числе взять готовые имплементации.\n1. [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/agent/react_agent/) - вдобавок можно посмотреть предыдущее задание, где он уже используется.\n2. [Langchain/Langgraph](https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/#code)\n3. Написать полностью свою реализацию\n\n\nНе забудьте, что очень важно описать задачу в промпте: нужно сказать, какие цели у агента и что он должен сделать. У функций должны быть говорящие описания, чтобы LLM без лишних проблем поняла, какие есть функции и когда их использовать. По всем вопросам можно обращаться в наш телеграм-чат в канал \"Tools & Agents\".\n","metadata":{"id":"cc9378f6"}},{"id":"5808fc02","cell_type":"code","source":"!pip install yfinance","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5a00d2f0","cell_type":"code","source":"import yfinance as yf\n\nstock = yf.Ticker(\"AAPL\") # посмотрим котировки APPLE\ndf = stock.history(period=\"1mo\")\ndf[[\"Open\", \"Close\"]]","metadata":{"id":"5a00d2f0","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:39:09.050616Z","iopub.execute_input":"2025-02-10T16:39:09.051034Z","iopub.status.idle":"2025-02-10T16:39:10.764892Z","shell.execute_reply.started":"2025-02-10T16:39:09.050998Z","shell.execute_reply":"2025-02-10T16:39:10.763572Z"}},"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"                                 Open       Close\nDate                                             \n2025-01-10 00:00:00-05:00  240.009995  236.850006\n2025-01-13 00:00:00-05:00  233.529999  234.399994\n2025-01-14 00:00:00-05:00  234.750000  233.279999\n2025-01-15 00:00:00-05:00  234.639999  237.869995\n2025-01-16 00:00:00-05:00  237.350006  228.259995\n2025-01-17 00:00:00-05:00  232.119995  229.979996\n2025-01-21 00:00:00-05:00  224.000000  222.639999\n2025-01-22 00:00:00-05:00  219.789993  223.830002\n2025-01-23 00:00:00-05:00  224.740005  223.660004\n2025-01-24 00:00:00-05:00  224.779999  222.779999\n2025-01-27 00:00:00-05:00  224.020004  229.860001\n2025-01-28 00:00:00-05:00  230.850006  238.259995\n2025-01-29 00:00:00-05:00  234.119995  239.360001\n2025-01-30 00:00:00-05:00  238.669998  237.589996\n2025-01-31 00:00:00-05:00  247.190002  236.000000\n2025-02-03 00:00:00-05:00  229.990005  228.009995\n2025-02-04 00:00:00-05:00  227.250000  232.800003\n2025-02-05 00:00:00-05:00  228.529999  232.470001\n2025-02-06 00:00:00-05:00  231.289993  233.220001\n2025-02-07 00:00:00-05:00  232.600006  227.630005\n2025-02-10 00:00:00-05:00  229.570007  228.910004","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>Close</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2025-01-10 00:00:00-05:00</th>\n      <td>240.009995</td>\n      <td>236.850006</td>\n    </tr>\n    <tr>\n      <th>2025-01-13 00:00:00-05:00</th>\n      <td>233.529999</td>\n      <td>234.399994</td>\n    </tr>\n    <tr>\n      <th>2025-01-14 00:00:00-05:00</th>\n      <td>234.750000</td>\n      <td>233.279999</td>\n    </tr>\n    <tr>\n      <th>2025-01-15 00:00:00-05:00</th>\n      <td>234.639999</td>\n      <td>237.869995</td>\n    </tr>\n    <tr>\n      <th>2025-01-16 00:00:00-05:00</th>\n      <td>237.350006</td>\n      <td>228.259995</td>\n    </tr>\n    <tr>\n      <th>2025-01-17 00:00:00-05:00</th>\n      <td>232.119995</td>\n      <td>229.979996</td>\n    </tr>\n    <tr>\n      <th>2025-01-21 00:00:00-05:00</th>\n      <td>224.000000</td>\n      <td>222.639999</td>\n    </tr>\n    <tr>\n      <th>2025-01-22 00:00:00-05:00</th>\n      <td>219.789993</td>\n      <td>223.830002</td>\n    </tr>\n    <tr>\n      <th>2025-01-23 00:00:00-05:00</th>\n      <td>224.740005</td>\n      <td>223.660004</td>\n    </tr>\n    <tr>\n      <th>2025-01-24 00:00:00-05:00</th>\n      <td>224.779999</td>\n      <td>222.779999</td>\n    </tr>\n    <tr>\n      <th>2025-01-27 00:00:00-05:00</th>\n      <td>224.020004</td>\n      <td>229.860001</td>\n    </tr>\n    <tr>\n      <th>2025-01-28 00:00:00-05:00</th>\n      <td>230.850006</td>\n      <td>238.259995</td>\n    </tr>\n    <tr>\n      <th>2025-01-29 00:00:00-05:00</th>\n      <td>234.119995</td>\n      <td>239.360001</td>\n    </tr>\n    <tr>\n      <th>2025-01-30 00:00:00-05:00</th>\n      <td>238.669998</td>\n      <td>237.589996</td>\n    </tr>\n    <tr>\n      <th>2025-01-31 00:00:00-05:00</th>\n      <td>247.190002</td>\n      <td>236.000000</td>\n    </tr>\n    <tr>\n      <th>2025-02-03 00:00:00-05:00</th>\n      <td>229.990005</td>\n      <td>228.009995</td>\n    </tr>\n    <tr>\n      <th>2025-02-04 00:00:00-05:00</th>\n      <td>227.250000</td>\n      <td>232.800003</td>\n    </tr>\n    <tr>\n      <th>2025-02-05 00:00:00-05:00</th>\n      <td>228.529999</td>\n      <td>232.470001</td>\n    </tr>\n    <tr>\n      <th>2025-02-06 00:00:00-05:00</th>\n      <td>231.289993</td>\n      <td>233.220001</td>\n    </tr>\n    <tr>\n      <th>2025-02-07 00:00:00-05:00</th>\n      <td>232.600006</td>\n      <td>227.630005</td>\n    </tr>\n    <tr>\n      <th>2025-02-10 00:00:00-05:00</th>\n      <td>229.570007</td>\n      <td>228.910004</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":89},{"id":"1fb6b85c","cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n@tool\ndef get_day_with_biggest_price_change(stock_ticker: str) -> str:\n    \"\"\"Finds a date in '%Y-%m-%d' format, that represents the day with the biggest open and close price change for the given stock ticker\"\"\"\n    stock = yf.Ticker(stock_ticker)\n    df = stock.history(period=\"1mo\")\n    change_series = (df.Open - df.Close).abs()\n    max_date = change_series.idxmax()\n    return str(max_date.date())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:39:30.523627Z","iopub.execute_input":"2025-02-10T16:39:30.524544Z","iopub.status.idle":"2025-02-10T16:39:30.535933Z","shell.execute_reply.started":"2025-02-10T16:39:30.524508Z","shell.execute_reply":"2025-02-10T16:39:30.534846Z"}},"outputs":[],"execution_count":90},{"id":"11e0f329","cell_type":"code","source":"@tool\ndef get_company_news(company_name: str, date_before: str) -> List[Dict[str, str]]:\n    \"\"\"Returns five (or less if impossible) news titles and descriptions about the company, that was published before the given date\"\"\"\n    news = []\n\n    api_key = \"1a9ea1edb0394f93a7b810601d7bcdcb\"\n    api_template = f\"https://newsapi.org/v2/everything?q={company_name}&apiKey={api_key}&to={date_before}\"\n    articles = requests.get(api_template).json()['articles']\n    for i in range(min(5, len(articles))):\n        if articles[i][\"title\"] != \"[Removed]\":\n            news.append({\"title\": articles[i][\"title\"], \"description\": articles[i][\"description\"]})\n\n    return news   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:39:33.126714Z","iopub.execute_input":"2025-02-10T16:39:33.127148Z","iopub.status.idle":"2025-02-10T16:39:33.135554Z","shell.execute_reply.started":"2025-02-10T16:39:33.127118Z","shell.execute_reply":"2025-02-10T16:39:33.134497Z"}},"outputs":[],"execution_count":91},{"id":"026d2b6b","cell_type":"markdown","source":"Для поиска новостей нам поможет https://newsapi.org/\nМожно легко получить свой ключ за короткую регистрацию, дается 1000 запросов в день, каждый запрос может включать в себя ключевое слово и промежуток дат. По бесплатному апи ключу дается ровно 1 месяц, что нам подходит.","metadata":{"id":"026d2b6b"}},{"id":"639e4310","cell_type":"code","source":"api_key = \"1a9ea1edb0394f93a7b810601d7bcdcb\" # ваш API ключ здесь!\napi_template = \"https://newsapi.org/v2/everything?q={keyword}&apiKey={api_key}&to={date_from}\"\n\narticles = requests.get(api_template.format(keyword=\"Apple\", api_key=api_key, date_from=\"2025-01-25\")).json()\n\nfor article in articles[\"articles\"]:\n    if article[\"title\"] != \"[Removed]\":\n        print(article[\"title\"])\n        print(article[\"description\"])\n        break","metadata":{"id":"639e4310","trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:39:37.872108Z","iopub.execute_input":"2025-02-10T16:39:37.872449Z","iopub.status.idle":"2025-02-10T16:39:38.129363Z","shell.execute_reply.started":"2025-02-10T16:39:37.872423Z","shell.execute_reply":"2025-02-10T16:39:38.128384Z"}},"outputs":[{"name":"stdout","text":"How to Use Apple’s Genmoji to Create New Kinds of Emojis\nApple Intelligence lets you generate custom emojis when the default options aren’t hitting it.\n","output_type":"stream"}],"execution_count":92},{"id":"fd6c1e29","cell_type":"code","source":"from langchain_together import ChatTogether\nfrom langchain_core.prompts import ChatPromptTemplate\n\nmodel = ChatTogether(model=model_70b)\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful financial analyst. Your aims are to find stock price for a compony, find relative news about the compony and build a connection between price changes and the news\"),\n        (\"user\", \"{input}\"),\n        (\"placeholder\", \"{agent_scratchpad}\"), # без этой строчки не работает, вроде это необходимость\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:40:42.751642Z","iopub.execute_input":"2025-02-10T16:40:42.752064Z","iopub.status.idle":"2025-02-10T16:40:42.882353Z","shell.execute_reply.started":"2025-02-10T16:40:42.752033Z","shell.execute_reply":"2025-02-10T16:40:42.881356Z"}},"outputs":[],"execution_count":93},{"id":"380207f4","cell_type":"code","source":"from langchain.agents import AgentExecutor, create_tool_calling_agent\n\nquery = \"What is the highest price change of Apple company stocks for a recent month before '2025-01-25'?\"\ntools = [get_day_with_biggest_price_change, get_company_news]\nagent = create_tool_calling_agent(model, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:41:10.370926Z","iopub.execute_input":"2025-02-10T16:41:10.371299Z","iopub.status.idle":"2025-02-10T16:41:10.600008Z","shell.execute_reply.started":"2025-02-10T16:41:10.371270Z","shell.execute_reply":"2025-02-10T16:41:10.599090Z"}},"outputs":[],"execution_count":94},{"id":"19a86fd5","cell_type":"code","source":"response = agent_executor.invoke({\"input\": query})\nprint(response['output'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:41:14.284663Z","iopub.execute_input":"2025-02-10T16:41:14.285060Z","iopub.status.idle":"2025-02-10T16:41:36.147017Z","shell.execute_reply.started":"2025-02-10T16:41:14.285028Z","shell.execute_reply":"2025-02-10T16:41:36.146132Z"}},"outputs":[{"name":"stdout","text":"The highest price change of Apple company stocks for a recent month before '2025-01-25' was on '2025-01-31'. \nHere are five news titles and descriptions about Apple that were published before '2025-01-25': \n\n1. How to Use Apple’s Genmoji to Create New Kinds of Emojis\n   Apple Intelligence lets you generate custom emojis when the default options aren’t hitting it.\n\n2. Parallels is testing x86 emulation on Apple silicon Macs\n   Parallels version 20.2 has added an early preview of x86 emulation support for Apple silicon Macs, with several limitations that mean most won’t want to use it.\n\n3. The 4.8-Star-Rated 2024 15-Inch Apple MacBook Air Is $250 Off on Amazon for MLK Weekend\n   The lightning-fast M3-powered Apple laptop is built for the all-new Apple Intelligence and has an incredible 18-hour battery life.\n\n4. Apple is pausing notification summaries for news in the latest iOS 18.3 beta\n   Apple has temporarily stopped showing notification summaries for news and entertainment apps in the iOS 18.3 beta, according to MacRumors and 9to5Mac.\n\n5. Samsung borrows from the Apple Wallet playbook with layaway and tap-to-send for Wallet\n   Alongside the launch of the Galaxy S25 series today, Samsung also made a slew of software updates that might not have gotten much attention during the keynote. Specifically, two updates are coming to Samsung Wallet that not only sound very similar to existing…\n","output_type":"stream"}],"execution_count":95},{"id":"e80cb6ed-3466-4406-bac1-37f31131f685","cell_type":"markdown","source":"## Все сработало!","metadata":{}}]}