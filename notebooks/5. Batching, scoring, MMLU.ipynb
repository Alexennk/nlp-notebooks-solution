{"cells":[{"cell_type":"code","execution_count":null,"id":"11861b12","metadata":{"id":"11861b12"},"outputs":[],"source":["from typing import List\n","\n","import torch\n","import torch.nn as nn\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","\n","# можете сменить на mps на макбуке, но лично у меня он криво работает\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"markdown","id":"ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3","metadata":{"id":"ca7a6c6c-64cc-4a1c-8b1e-3ccee36396d3"},"source":["# Знакомство с Transformers"]},{"cell_type":"markdown","id":"a3df5693","metadata":{"id":"a3df5693"},"source":["## Создание модели и предсказание следующего токена - 5 баллов\n","Нужно создать модель через `AutoModelForCausalLM`, создать токенайзер через `AutoTokenizer` и получить следующий токен через жадную генерацию!\n","\n","Для загрузки модели и токенайзера вам помогут функции `.from_pretrained`\n","\n","**Внимание** на каких-то из функций далее у вас может кончаться видеопамять из-за хранения активаций. Чтобы этого не происходило рекомендуется все вычисления оборачивать в контекстный менеджер `with torch.no_grad()`"]},{"cell_type":"code","execution_count":null,"id":"6ec7e08b","metadata":{"id":"6ec7e08b"},"outputs":[],"source":["model_name = \"openai-community/gpt2\"\n","model = AutoModelForCausalLM # Ваш код здесь\n","tokenizer = AutoTokenizer # ваш код здесь\n","\n","\n","text = \"This is a sample text\"\n","\n","# Нужно преобразовать text с помощью tokenizer() и подать это в model.forward() (он же просто model())\n","# после этого мы получим logits [batch_size = 1, seq_len, d_model]\n","# По этому тензору нужно предсказать следующее слово!\n","\n","inputs = tokenizer(text, ...)\n","\n","outputs = model(...)\n","logits = ...\n","next_token_idx: int = ...\n","\n","\n","next_token = tokenizer.decode([next_token_idx])\n","\n","assert next_token.strip() == \"file\"\n","\n"]},{"cell_type":"markdown","id":"e6809813","metadata":{"id":"e6809813"},"source":["## Используем Generate - 5 баллов\n","\n","Мы с вами помним про различные виды сэмплинга - top_k, top_p, temperature,frequency penalty.\n","Отличная новость заключается в том, что нам не нужно все это писать самим! Оно уже включено в [GenerationMixin](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#generation), от которого наследуются модели для генерации текста.\n","\n","Для генерации есть функция [generate](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/text_generation#transformers.GenerationMixin.generate)\n","\n","Ваша задача написать для модели выше генерацию по тексту с:\n","* Температурой - 0.9\n","* Top-K - 20\n","* Repetition Penalty (Frequency Penalty) - 1.2\n","* максимальное число новых токенов - 10\n"]},{"cell_type":"code","execution_count":null,"id":"a6b62dbf","metadata":{"id":"a6b62dbf"},"outputs":[],"source":["text = \"This is still a sample text, but\"\n","inputs = tokenizer(text, ...)\n","\n","results = []\n","for i in range(10):\n","    gens = model.generate(\n","        ...\n","    )\n","    genertaion: str = ... # сгенерированный текст\n","    results.append(generation)\n","\n","assert len(set(results)) > 1, \"Все генерации получились одинаковыми, проверьте опции генерации и флаг do_sample!\""]},{"cell_type":"markdown","id":"8b90512b-9420-45b3-9f4c-22fb5fa1bfc7","metadata":{"id":"8b90512b-9420-45b3-9f4c-22fb5fa1bfc7"},"source":["## Generate Batched - 5\n","Теперь давайте жадно сгенерируем текст, но забатчуем несколько сэмплов. До этого мы всегда генерировали по батчу размера 1, поэтому у нас не было паддингов!\n","\n","Когда появляется несколько текстов разной длины, то появляются и паддинги.\n","\n","Представим себе ситуцию, что у нас батч из двух элементов длины 2 и 5 (токен -1 будет выступать в качестве паддинга **только для удобства визуализации**).\n","\n","Тогда\n","\n","```python\n","input_ids = [\n","    [3, 2, -1, -1, -1]\n","    [5, 6,  7,  1,  2]\n","]\n","attention_mask = [\n","    [1, 1, 0, 0, 0],\n","    [1, 1, 1, 1, 1]\n","]\n","```\n","\n","Представим, что мы сгенерировали еще один токен, тогда\n","\n","```python\n","input_ids = [\n","    [3, 2, -1, -1, -1, 7]\n","    [5, 6,  7,  1,  2, 8]\n","]\n","attention_mask = [\n","    [1, 1, 0, 0, 0, 1],\n","    [1, 1, 1, 1, 1, 1]\n","]\n","```\n","\n","Получается, что у нас паддинги в маске возникают посередине. Мы не будем заниматься реализацией своего алгоритма генерации здесь, но отметим, что добавление паддинга слева значительно упрощает этот процесс.\n","Тогда исходная последовательность будет:\n","\n","```python\n","input_ids = [\n","    [-1, -1, -1, 3, 2]\n","    [ 5,  6,  7, 1, 2]\n","]\n","attention_mask = [\n","    [0, 0, 0, 1, 1],\n","    [1, 1, 1, 1, 1]\n","]\n","```\n","\n","и после генерации следующего токена\n","\n","```python\n","input_ids = [\n","    [-1, -1, -1, 3, 2, 7]\n","    [ 5,  6,  7, 1, 2, 8]\n","]\n","attention_mask = [\n","    [0, 0, 0, 1, 1, 1],\n","    [1, 1, 1, 1, 1, 1]\n","]\n","```\n","\n","В качестве задания давайте соберем батч с левым паддингом и проверим, что жадная генерация (10 токенов) совпадает с генерацией на текстах по отдельности!\n","\n","Для этого нам придется использовать параметр padding_side в конструкторе токенизатора."]},{"cell_type":"code","execution_count":null,"id":"5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1","metadata":{"id":"5db4cd76-b37b-4fd4-9cf8-8f76e04ae7a1"},"outputs":[],"source":["tokenizer = AutoTokenizer # ваш код здесь\n","tokenizer.pad_token_id = tokenizer.eos_token_id"]},{"cell_type":"code","execution_count":null,"id":"1bd38bdc-3e5e-400d-8815-e9c08a757c03","metadata":{"scrolled":true,"id":"1bd38bdc-3e5e-400d-8815-e9c08a757c03"},"outputs":[],"source":["texts = [\"This is a sample text\", \"I'm really tired and this is just about\"]\n","\n","# Внимание! В данном задании нужна жадная генерация!\n","\n","# Соберите оба текста в один батч и положите результаты генерации в\n","# batched_generations\n","batched_generations: List[str] = []\n","\n","....\n","\n","# Пройдитесь по каждому сэмплу по отдельности и положите результаты генерации\n","# в single_generations\n","single_generations: List[str] = []\n","\n","...\n","\n","assert len(batched_generations) == 2 and len(single_generations) == 2\n","for s, b in zip(batched_generations, single_generations):\n","    assert s == b"]},{"cell_type":"markdown","id":"f5da008c-3653-40d5-89ba-cd831352fd3d","metadata":{"id":"f5da008c-3653-40d5-89ba-cd831352fd3d"},"source":["# Скоринг, Perplexity - 10 баллов\n","\n","Можно не только генерировать текст. Вспомним, что выдает после lm_head - вектор `[batch_size, seq_len, vocab_size]`, где для каждый вектор `[vocab_size]` это распределение вероятностей по следующему токену!\n","\n","Опустим размерность batch_size=1 для удобства, seq_len = 4. Пусть у нас есть текст `bos мама мыла раму` (`bos` спецсимвол для начала текста)\n","\n","Тогда вероятность этого текста расписывается через произведение условных вероятностей:\n","\n","```\n","P(bos мама мыла раму) = P(мама | bos) * P(мыла | bos мама) * P(раму| bos мама мыла)\n","```\n","\n","Т.е. это вероятность слова при условии его левого контекста.\n","Зачастую ее обозначают как $P(x_i|x_{<i})$ где $x_i$ - i-е слово, $x_{<i}$ - контекст $[x_1, x_2, x_3, ... x_{i-1}]$\n","Эти вероятности можно взять из выходного вектора!\n","\n","Давайте попробуем подсчитать вероятность и perplexity текстов!\n","perplexity как и вероятность мера того насколько модель \"уверена\" в тексте, т.е. насколько по оценки ее параметрами данный текст вероятен.\n","\n","$$Perplexity(X) = exp(-\\frac {1} {N} \\sum_{i}^{N} log P(x_i | x_{<i}))$$\n","\n","В этом задании нужно:\n","1. Посчитать вероятность **text**\n","2. Посчитать перплексию **text**\n","\n","Еще одна важная деталь:\n","работать с вероятностями плохо. Т.к. вероятность представляет собой число от 0 до 1, то при перемножении десятков или даже сотен таких числе теряется точность!\n","Для этого от произведения вероятностей берут логарифм и получают logprobs - логарифмы вероятностей. Их можно складывать, по свойству логарифма логарифм произведения равен произведению логарифма.\n","\n","$$ p = p_1 * p_2 * p_3 $$\n","$$log(p) = log (p_1) + log (p_2) + log (p_3)$$\n","$$exp(log (p)) = p = exp(log (p_1) + log (p_2) + log (p_3)) = exp (log (p_1 * p_2 * p_3)) = p_1 * p_2 * p_3$$\n","\n","В pytorch для этого есть `torch.log_softmax`, который считается численно стабильно!"]},{"cell_type":"code","execution_count":null,"id":"e1c7ba39-a451-43a2-ac55-629c99259abe","metadata":{"id":"e1c7ba39-a451-43a2-ac55-629c99259abe"},"outputs":[],"source":["print(f\"Beginning of sentence (BOS) token = `{tokenizer.bos_token}`\")\n","print(f\"End of sentence (EOS) token  = `{tokenizer.eos_token}`\")\n","text = \"<|endoftext|>I'm so very tired of this<|endoftext|>\"\n","\n","inputs = tokenizer(text, ...)\n","\n","\n","\n","with torch.no_grad():\n","    logits = model(...).logits\n","    ...\n","    # ваш код здесь!\n","    # 1. Нужно обрезать logits по длине, т.к. для предсказаний по последнему токену нечего считать\n","    # 2. Превращаем logits в log_probs\n","    # 3. Берем вероятности следующих токенов, т.к. по вектору i-й позиции мы предсказываем токен на позиции (i + 1)\n","    # для этого нам поможет torch.gather\n","    # 4. Считаем вероятности и perplexity!\n","\n","\n","# должно получиться что-то около 2.1783e-14 для вероятности и около 51 для ppl"]},{"cell_type":"markdown","id":"4f244eac-7cb1-4689-8adc-46662891e657","metadata":{"id":"4f244eac-7cb1-4689-8adc-46662891e657"},"source":["# Вопросы - 5 баллов\n","\n","**Ответьте на вопрсоы текстом прямо здесь!**\n","\n","\n","1. Какое значение P(X) вероятности текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n","2. Какое значение перплексии текста самое \"лучшее\" в том смысле, что модель максимально уверена в этом тексте и скорее всего его сгенерирует.\n","\n"]},{"cell_type":"markdown","id":"5ddd5038-620b-48bb-bbc1-db3729141d78","metadata":{"id":"5ddd5038-620b-48bb-bbc1-db3729141d78"},"source":["# Chat-Models"]},{"cell_type":"markdown","id":"599c7530-a7ce-4d23-abaf-2b0cec87301e","metadata":{"id":"599c7530-a7ce-4d23-abaf-2b0cec87301e"},"source":["# Формат - 5 баллов\n","Как мы обсуждали на лекции, все chat-модели принимают входы в своем особом формате.\n","Он может быть описан текстом, а может быть заложен в шаблон, который доступен через `tokenizer.apply_chat_template`"]},{"cell_type":"code","execution_count":null,"id":"7f5fe593-63a8-406d-9678-6d805c180670","metadata":{"id":"7f5fe593-63a8-406d-9678-6d805c180670"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\")\n","model = AutoModelForCausalLM.from_pretrained(\"NousResearch/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.half).to(device)"]},{"cell_type":"code","execution_count":null,"id":"7d8dad11-6811-49ab-ad3d-8ac7de103828","metadata":{"id":"7d8dad11-6811-49ab-ad3d-8ac7de103828"},"outputs":[],"source":["def move_to_device(d):\n","    for k, v in d.items():\n","        d[k] = v.to(device)\n","    return d"]},{"cell_type":"markdown","id":"503f15fd-576a-4e8e-917a-74df01a944f4","metadata":{"id":"503f15fd-576a-4e8e-917a-74df01a944f4"},"source":["Давайте посмотрим, как chat модель отработает на обычном тексте. Используйте для генерации сэмплинг и kv cache, выведите 5 результатов генерации."]},{"cell_type":"code","execution_count":null,"id":"7134f0bb-1ee4-4508-a26d-5326ea96562b","metadata":{"id":"7134f0bb-1ee4-4508-a26d-5326ea96562b"},"outputs":[],"source":["text = \"hello how are you\"\n","inputs = tokenizer(text, ...)\n","\n","for i in range(5):\n","    # model.generate...\n","    generated_text = ...\n","    print(generated_text)\n","    print(\"====\" * 3)\n"]},{"cell_type":"markdown","id":"3fd50470-64b9-4a21-8748-0e9c5ea439fc","metadata":{"id":"3fd50470-64b9-4a21-8748-0e9c5ea439fc"},"source":["Видим, что текст зачастую выходит мусорный. Это потому что формат входных данных сильно отличается от того, что модель видела на обучении.\n","Как мы уже обсуждали, у всех chat-моделей свой формат. Где-то он описан просто словами, где-то он заложен в токенайзер. Мы рассмотрим как раз такой случай - за нас есть удобно написанная функция `apply_chat_template`. Давайте используем ее, чтобы получить префикс для генерации модели.\n","\n","Не забудьте про опцию add_generation_prefix - она добавляет часть формата, после которой ожидается ответ модели!"]},{"cell_type":"code","execution_count":null,"id":"e79a3701-c80f-4b90-90bd-fa010e32ea36","metadata":{"id":"e79a3701-c80f-4b90-90bd-fa010e32ea36"},"outputs":[],"source":["messages = [\n","    {\"role\": \"user\", \"content\": \"hello\"},\n","    {\"role\": \"assistant\", \"content\": \"I'm good. How can I help you today\"},\n","    {\"role\": \"user\", \"content\": \"I love you\"},\n","]\n","\n","prefix = tokenizer.apply_chat_template(...)\n","\n","reference = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n","\n","hello<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\n","I'm good. How can I help you today<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","I love you<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n","\n","assert prefix.strip() == reference.strip()"]},{"cell_type":"markdown","id":"048b8882-bec0-4bf4-b6fb-30e727d095c6","metadata":{"id":"048b8882-bec0-4bf4-b6fb-30e727d095c6"},"source":["Давайте посмотрим, что нам ответит модель!"]},{"cell_type":"code","execution_count":null,"id":"4284b18d-4f9b-4e7d-b3ea-bb365e90093c","metadata":{"id":"4284b18d-4f9b-4e7d-b3ea-bb365e90093c"},"outputs":[],"source":["inputs = tokenizer(prefix, ...)\n","model.generate...\n","print(...)"]},{"cell_type":"markdown","id":"a72482f3-c296-46f3-851c-57b4f91a717b","metadata":{"id":"a72482f3-c296-46f3-851c-57b4f91a717b"},"source":["## Benchmark - 15"]},{"cell_type":"markdown","id":"52f422a9-c2ee-4c17-8aee-1830f1d143e6","metadata":{"id":"52f422a9-c2ee-4c17-8aee-1830f1d143e6"},"source":["Перед нами датасет MMLU - датасет вопросов и ответов в стиле multiple choice.\n","* question - вопрос\n","* choices - варианты ответа\n","* answer - номер правильного ответа"]},{"cell_type":"code","execution_count":null,"id":"530d1721-6623-4ca6-816c-d4f90203ceb2","metadata":{"id":"530d1721-6623-4ca6-816c-d4f90203ceb2","outputId":"737e99f7-c31e-4a94-9df4-a881ea514666"},"outputs":[{"data":{"text/plain":["{'question': 'What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices?',\n"," 'subject': 'global_facts',\n"," 'choices': ['About $300', 'About $3k', 'About $8k', 'About $15k'],\n"," 'answer': 1}"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import load_dataset\n","mmlu = load_dataset(\"cais/mmlu\", \"global_facts\", split=\"test\")\n","mmlu[1]"]},{"cell_type":"markdown","id":"fca61a91-5784-44f3-af9b-72250f8d58a4","metadata":{"id":"fca61a91-5784-44f3-af9b-72250f8d58a4"},"source":["Наша задача здесь решить задачу многоклассовой классификации.\n","Для этого нужно посчитать\n","$$P(choices_i | question)$$\n","т.е. для посчитать вероятность каждого варианта ответа для вопроса. Мы это уже делали кодом выше!\n","\n","После этого давайте брать самый вероятный ответ и считать, что модель его выбрала.\n","После этого давайте посчитаем accurracy, т.е. долю правильных ответов.\n","Вместо вероятностей для подсчета лучше использовать logprobs.\n","\n","Итого, что нужно сделать:\n","1. Пройтись по датасету, для каждого question и каждого из соответствующих choices получить самый вероятный ответ.\n","2. Посчитать итоговый accuracy\n","\n","**Важно**\n","1. Выше мы уже написали скоринг текста с помощью LLM, для этого задания можно адаптировать функцию.\n","2. Если делаете варианты с батчеванием помните: длины choices могут быть разными! Нужно не считать вероятности по паддингам. В этом нам помогут attention_masks из выходов `tokenizer()`\n","3. В данном задании для простоты мы игнорируем формат ответа llama3 и делаем скоринг по f\"{question} {answer}\"\n","\n","\n","Попробуйте для начала написать вариант со скорингом для батча размера 1, а потом для батча размера 3 или 5. Код должен корректно работать для батча любого размера и выдавать одинаковую итоговую точность.\n","\n","За задание, в котором код работает только с батчом размера 1, 2, 4 можно получить **только 10 баллов**"]},{"cell_type":"code","execution_count":null,"id":"4a4126e2-c463-404d-a3b5-5361f744242e","metadata":{"id":"4a4126e2-c463-404d-a3b5-5361f744242e","outputId":"3bd51c60-2716-4a18-9c58-b2dff1ef9319"},"outputs":[{"name":"stdout","output_type":"stream","text":["As of 2016, about what percentage of adults aged 18 years or older were overweight? 40%\n","As of 2016, about what percentage of adults aged 18 years or older were overweight? 80%\n","What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices? About $300\n","What was GDP per capita in the United States in 1850 when adjusting for inflation and PPP in 2011 prices? About $3k\n"]}],"source":["def sample_to_texts(sample):\n","    return [sample[\"question\"] + \" \" + answer for answer in sample[\"choices\"]]\n","\n","all_samples_formatted = sum([sample_to_texts(sample) for sample in mmlu], [])\n","print(*all_samples_formatted[2:6], sep=\"\\n\")\n","# ваш код здесь!"]},{"cell_type":"code","execution_count":null,"id":"9fe9c63f-5d9e-4204-b1a9-d93ca19c1888","metadata":{"id":"9fe9c63f-5d9e-4204-b1a9-d93ca19c1888"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"8a86188a","metadata":{"id":"8a86188a"},"source":["# Вопросы - 5 баллов"]},{"cell_type":"markdown","id":"76c440af-dfc0-460d-a113-3fab3fefa361","metadata":{"id":"76c440af-dfc0-460d-a113-3fab3fefa361"},"source":["**Ответьте на следующие вопросы (5 баллов в сумме)**:\n","1. Как влияет длина ответа на вероятность ответа при скоринге? Если есть какие-либо проблемы, как бы вы с этим боролись.\n","2. Если к началу каждого ответа добавилить метки A) B) C) D) станет ли модель отвечать лучше или хуже?\n","Стоит ли по-вашему добавлять эти метки?\n"]},{"cell_type":"code","execution_count":null,"id":"3835f1a9-05fb-440f-93b4-01c53419eb96","metadata":{"id":"3835f1a9-05fb-440f-93b4-01c53419eb96"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}