{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Домашнее задание (50 баллов)","metadata":{"collapsed":false,"id":"uWW89Nr0a_X_","jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"В этом домашнем задании вы познакомитесь с основами NLP, научитесь обрабатывать тексты.\n\nВ местах, где используется `...` (elipsis), требуется заменить его на код.\n\nУстановим необходимые зависимости:","metadata":{"collapsed":false,"id":"lvmdd1Bqa_YD","jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"!pip install -U pip\n!pip install nltk tqdm seqeval scikit-learn datasets numpy","metadata":{"ExecuteTime":{"end_time":"2024-11-27T19:21:30.570036Z","start_time":"2024-11-27T19:21:27.452197Z"},"execution":{"iopub.status.busy":"2025-01-30T14:38:16.576845Z","iopub.execute_input":"2025-01-30T14:38:16.577196Z","iopub.status.idle":"2025-01-30T14:38:27.797790Z","shell.execute_reply.started":"2025-01-30T14:38:16.577169Z","shell.execute_reply":"2025-01-30T14:38:27.796229Z"},"id":"83ojVxoPa_YE","pycharm":{"name":"#%%\n"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (25.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk) (1.17.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from typing import List, Dict, Tuple, Callable","metadata":{"ExecuteTime":{"end_time":"2024-11-27T19:21:40.162946Z","start_time":"2024-11-27T19:21:40.157819Z"},"execution":{"iopub.status.busy":"2025-01-30T14:38:36.080564Z","iopub.execute_input":"2025-01-30T14:38:36.081895Z","iopub.status.idle":"2025-01-30T14:38:36.090126Z","shell.execute_reply.started":"2025-01-30T14:38:36.081743Z","shell.execute_reply":"2025-01-30T14:38:36.089064Z"},"id":"6cAALhDFa_YH","trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Токенизация (15 баллов)\n\nТокенизация - это процесс преобразования текста в набор токенов.\nНаивная реализация разбивает текст по пробелам. Более умные реализации учитывают пунктуацию.\n\n### Библиотека NLTK (2 балла)\n\nНаучимся работать с токенизацией NLTK, где уже [реализована](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize) работа с пунктуацией.\n\nhttps://www.nltk.org/","metadata":{"collapsed":false,"id":"KFpJwwpMa_YH","jupyter":{"outputs_hidden":false},"pycharm":{"name":"#%%\n"}}},{"cell_type":"code","source":"import nltk","metadata":{"execution":{"iopub.status.busy":"2025-01-30T14:38:38.903504Z","iopub.execute_input":"2025-01-30T14:38:38.904921Z","iopub.status.idle":"2025-01-30T14:38:43.300640Z","shell.execute_reply.started":"2025-01-30T14:38:38.904781Z","shell.execute_reply":"2025-01-30T14:38:43.297751Z"},"id":"_gTttD-Ha_YI","trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# https://www.nltk.org/nltk_data/\nnltk.download(\"punkt\")\nnltk.download('punkt_tab')\n\n\ndef tokenize(text: str, language: str = \"english\") -> List[str]:\n    return nltk.tokenize.word_tokenize(text, language)\n\nassert tokenize(\"\") == []\nassert tokenize(\"Hello, world!\") == [\"Hello\", \",\", \"world\", \"!\"]\nassert tokenize(\"EU rejects German call to boycott British lamb.\") == [\"EU\", \"rejects\", \"German\", \"call\", \"to\", \"boycott\", \"British\", \"lamb\", \".\"]","metadata":{"execution":{"iopub.status.busy":"2025-01-30T14:38:44.392810Z","iopub.execute_input":"2025-01-30T14:38:44.393392Z","iopub.status.idle":"2025-01-30T14:38:44.626559Z","shell.execute_reply.started":"2025-01-30T14:38:44.393316Z","shell.execute_reply":"2025-01-30T14:38:44.625438Z"},"id":"_gTttD-Ha_YI","trusted":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Нормализация (3 балла)\n\nДобавим нормализацию после токенизации. Пробуем [лемматизацию](https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.wordnet.WordNetLemmatizer) , [стемминг](https://www.nltk.org/api/nltk.stem.snowball.html#nltk.stem.snowball.EnglishStemmer) и [юникод](https://docs.python.org/3/library/unicodedata.html#unicodedata.normalize) нормализацию. Напишем функцию, которая будет принимать на вход токен после токенизации, нормализовать в NFC юникод форму, переводит в нижний регистр, лемматизирует слово и, если слово не изменилось после лемматизации, применяет стемминг.\n\n\nСоздайте функцию `normalize`:\n   - Функция `normalize` должна принимать строку `token` и возвращать нормализованный токен.\n   - Примените к токену Unicode нормализацию с помощью `unicode_nfc_normalizer`.\n   - Преобразуйте токен в нижний регистр.\n   - Примените лемматизацию с помощью `lemmatizer`.\n   - Если лемматизированный токен отличается от исходного, верните его. В противном случае, примените стемминг с помощью `stemmer` и верните результат.","metadata":{"id":"PGvw3rC1a_YJ"}},{"cell_type":"code","source":"import unicodedata\n\nnltk.download('wordnet')\n\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2025-01-30T14:38:48.897281Z","iopub.execute_input":"2025-01-30T14:38:48.897726Z","iopub.status.idle":"2025-01-30T14:38:48.979643Z","shell.execute_reply.started":"2025-01-30T14:38:48.897694Z","shell.execute_reply":"2025-01-30T14:38:48.978361Z"},"id":"O2jkgODVa_YK","trusted":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# питон 3.10 ужасен и в нем nltk 3.2.4 не умеет распаковывать zip архивы!\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:38:51.378416Z","iopub.execute_input":"2025-01-30T14:38:51.378796Z","iopub.status.idle":"2025-01-30T14:38:54.855970Z","shell.execute_reply.started":"2025-01-30T14:38:51.378766Z","shell.execute_reply":"2025-01-30T14:38:54.854711Z"}},"outputs":[{"name":"stdout","text":"Archive:  /usr/share/nltk_data/corpora/wordnet.zip\nreplace /usr/share/nltk_data/corpora/wordnet/lexnames? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"stemmer = EnglishStemmer()\nlemmatizer = WordNetLemmatizer()\n\n\ndef normalize(token: str) -> str:\n    \"\"\"\n    Нормализует токен, применяя Unicode нормализацию, преобразование в нижний регистр,\n    лемматизацию и стемминг при необходимости.\n\n    :param token: Токен для нормализации\n    :return: Нормализованный токен\n    \"\"\"\n    normalized_token = unicodedata.normalize('NFC', token)\n    lower_case_token = normalized_token.lower()\n    lemmatized_token = lemmatizer.lemmatize(lower_case_token)\n    if lemmatized_token != lower_case_token:\n        return lemmatized_token\n    return stemmer.stem(lemmatized_token)\n\n\ntest_tokens = [\"Worlds\", \"churches\", \"Helping\"]\nassert [normalize(token) for token in test_tokens] == [\"world\", \"church\", \"help\"]","metadata":{"execution":{"iopub.status.busy":"2025-01-30T14:38:57.516058Z","iopub.execute_input":"2025-01-30T14:38:57.516573Z","iopub.status.idle":"2025-01-30T14:38:59.288983Z","shell.execute_reply.started":"2025-01-30T14:38:57.516530Z","shell.execute_reply":"2025-01-30T14:38:59.287789Z"},"id":"O2jkgODVa_YK","trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Добавляем Словарь (10 баллов)","metadata":{"id":"YSvfV7kta_YL"}},{"cell_type":"markdown","source":"Современные токенайзеры не только разбивают строки на токены, но и преобразуют последовательность токенов в последовательность числел. Объединим функцию токенизации, нормализации и отображения из токенов в индексы в один объект токенайзера.\n\nНапишите класс `Tokenizer` для токенизации и нормализации текста.\n\nПостроение словаря:\n   - Создайте метод `_build_vocabulary`, который принимает список текстов `texts` и обновляет словарь токенов.\n   - Для каждого текста:\n     - Токенизируйте и нормализуйте текст.\n     - Обновите счетчик вхождений слов.\n   - Для каждого слова, которое встречается не менее `min_count` раз, добавьте слово в словарь `word2idx` и список `idx2word`.\n\nКодирование и декодирование:\n   - Создайте метод `encode_word`, который принимает слово `word` и возвращает его индекс с применением нормализации.\n   - Создайте метод `encode`, который принимает текст `text` и возвращает список индексов токенов.\n   - Создайте метод `decode`, который принимает список индексов `input_ids` и возвращает текст, вставляя пробелы между токенами.\n\n> Note: для функций, которые могут долго исполнятся (`_build_vocab`), рекомендуется использовать библиотеку tqdm.","metadata":{"id":"2dnoIj13a_YM"}},{"cell_type":"code","source":"from collections import Counter\nfrom tqdm.notebook import tqdm\n\n\nclass Tokenizer:\n    def __init__(\n            self,\n            texts: List[str],\n            tokenize_fn: Callable[[str], List[str]] = tokenize,\n            normalize_fn: Callable[[str], str] = lambda token: token,\n            min_count: int = 1\n    ) -> None:\n        \"\"\"\n        Инициализация токенизатора.\n\n        :param texts: список текстов для построения словаря\n        :param tokenize_fn: функция для токенизации текста\n        :param normalize_fn: функция для нормализации токенов\n        :param min_count: минимальное количество вхождений слова для включения в словарь\n        \"\"\"\n        self.min_count = min_count\n        self.tokenize = tokenize_fn\n        self.normalize = normalize_fn\n        self.word2idx = {\"<PAD>\": 0, \"<BOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.unk_token_id = 3\n        self.idx2word = [\"<PAD>\", \"<BOS>\", \"<EOS>\", \"<UNK>\"]\n        self.word2count = Counter()\n        self._build_vocabulary(texts)\n\n    def _build_vocabulary(self, texts: List[str]):\n        \"\"\"\n        Построение словаря на основе списка текстов.\n\n        :param texts: список текстов\n        \"\"\"\n        tokens_list = [self.tokenize(text) for text in tqdm(texts, desc=\"Tokenizing texts\")]\n        normalized_tokens = [self.normalize(token) for tokens in tqdm(tokens_list, desc=\"Normalizing tokens\") for token in tokens]\n        all_tokens = self.idx2word + normalized_tokens\n        self.word2count = Counter(all_tokens)\n\n        for token in self.idx2word:\n            self.word2count[token] = self.min_count # чтобы дальше не удалить специальные токены\n\n        index = 0\n        self.idx2word = []\n        self.word2idx = {}\n        for token in tqdm(all_tokens, desc=\"Building vocabulary\"):\n            if token not in self.word2idx and self.word2count[token] >= self.min_count:\n                self.word2idx[token] = index\n                self.idx2word.append(token)\n                index += 1\n\n\n    def encode_word(self, text: str) -> int:\n        \"\"\"\n        Кодирование слова в индекс с применением нормализации.\n\n        :param text: слово\n        :return: индекс слова\n        \"\"\"\n        token = self.normalize(text)\n        if token in self.word2idx:\n            return self.word2idx[token]\n        else:\n            return self.word2idx['<UNK>'] # for tf-idf\n\n    def encode(self, text: str) -> List[int]:\n        \"\"\"\n        Кодирование текста в набор индексов.\n\n        :param text: текст\n        :return: набор индексов токенов\n        \"\"\"\n        tokens = [self.normalize(token) for token in self.tokenize(text)]\n        return [self.encode_word(word) for word in tokens]\n\n    def decode(self, input_ids: List[int]) -> str:\n        \"\"\"\n        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n\n        :param input_ids: набор индексов токенов\n        :return: текст\n        \"\"\"\n        return ' '.join([self.idx2word[id] for id in input_ids])\n\n    def __len__(self) -> int:\n        \"\"\"\n        Возвращает количество уникальных токенов в словаре.\n\n        :return: количество уникальных токенов\n        \"\"\"\n        return len(self.word2idx)\n\n    def __contains__(self, item: str) -> bool:\n        \"\"\"\n        Проверяет, содержится ли слово в словаре.\n\n        :param item: слово\n        :return: True, если слово содержится в словаре, иначе False\n        \"\"\"\n        return item in self.word2idx\n\n    def __str__(self):\n        \"\"\"\n        Возвращает строковое представление словаря.\n\n        :return: строковое представление словаря\n        \"\"\"\n        return str(self.word2idx)","metadata":{"id":"SGYFZQwZa_YN","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:39:02.095358Z","iopub.execute_input":"2025-01-30T14:39:02.095764Z","iopub.status.idle":"2025-01-30T14:39:02.364156Z","shell.execute_reply.started":"2025-01-30T14:39:02.095732Z","shell.execute_reply":"2025-01-30T14:39:02.362705Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"corpus = [\"Hello, world!\", \"I love Python!\"]\n\ntokenizer = Tokenizer(corpus, min_count=1)\nencoded = tokenizer.encode(\"Hello, Python! I love you\")\nassert tokenizer.decode(encoded) == \"Hello , Python ! I love <UNK>\"\n\ntokenizer = Tokenizer(corpus, normalize_fn=normalize)\nencoded = tokenizer.encode(\"Hello, Python! I loved you\")\nassert tokenizer.decode(encoded) == \"hello , python ! i love <UNK>\"","metadata":{"id":"pHbJy1Oga_YO","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:39:06.434775Z","iopub.execute_input":"2025-01-30T14:39:06.435134Z","iopub.status.idle":"2025-01-30T14:39:06.559171Z","shell.execute_reply.started":"2025-01-30T14:39:06.435101Z","shell.execute_reply":"2025-01-30T14:39:06.557979Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing texts:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f8780f5f39e4766b86259a0bb4c7f9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Normalizing tokens:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3acd6dfed4f94a7cb16e19494293859b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b136d7a567f449ab1b79b9716904afa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing texts:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b19eb4407534d1a847936be4123a41a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Normalizing tokens:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f72eac829947477e952460e26a3091e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae03754c27034d59a22be6fb0cb18e27"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## TF-IDF (20 баллов)\n\n\n### Класс TFIDF (10 баллов)\n\nСоздайте класс `TFIDF` для вычисления TF-IDF значений. Формулы для подсчёта TF и IDF можно выбрать [тут](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n\nОбучение модели должно осуществляться с помощью метода `fit`, который принимает список строк `docs` и обучает модель на этом корпусе, вызывая метод `add_doc` для каждого документа.\n\nПредсказание TF-IDF значений:\n   - Создайте метод `predict`, который принимает список строк `docs` и возвращает матрицу TF-IDF значений.\n   - Для каждого документа:\n     - Токенизируйте документ.\n     - Вычислите TF для каждого термина.\n     - Вычислите IDF для каждого термина.\n     - Заполните матрицу TF-IDF значений.\n   - Нормализуйте строки матрицы, чтобы сумма значений в каждой строке была равна 1.\n\n> Важно! Не забудьте убрать `<UNK>` токен во  время подсчёта TF-IDF\n\nДля функций, которые могут долго исполнятся (`fit`, `predict`), рекомендуется использовать библиотеку tqdm.","metadata":{"id":"veHdNWrGa_YO"}},{"cell_type":"code","source":"from collections import Counter\nfrom typing import List\nimport numpy as np\n\n\nclass TFIDF:\n    def __init__(self, tokenizer: Tokenizer, default_idf = 1.0) -> None:\n        \"\"\"\n        Инициализация TFIDF.\n\n        :param tokenizer: токенизатор для преобразования текста в токены\n        :param default_idf: значение IDF для неизвестных токенов\n        \"\"\"\n        self.tokenizer = tokenizer\n        self.num_docs = 0\n        self.term2num_docs = [0 for _ in self.tokenizer.word2idx]  # для подсчёта IDF, n_t\n        self.default_idf = default_idf\n\n    @property\n    def vocab_size(self) -> int:\n        \"\"\"\n        Возвращает размер словаря.\n\n        :return: размер словаря\n        \"\"\"\n        return len(self.tokenizer)\n\n    def add_doc(self, doc: str) -> None:\n        \"\"\"\n        Добавляет документ в модель TFIDF.\n\n        :param doc: документ для добавления\n        \"\"\"\n        tokens = set(self.tokenizer.encode(doc))\n        if self.tokenizer.unk_token_id in tokens:\n            tokens.remove(self.tokenizer.unk_token_id)\n\n        for token in tokens:\n            self.term2num_docs[token] += 1\n        self.num_docs += 1\n\n    def fit(self, docs: List[str]) -> None:\n        \"\"\"\n        Обучает модель TFIDF на корпусе docs.\n\n        :param docs: корпус для обучения\n        \"\"\"\n        self.num_docs = 0\n        self.term2num_docs = [0 for _ in self.tokenizer.word2idx]   # Сбрасываем счётчики перед обучением\n\n        for doc in tqdm(docs, desc=\"Training TF-IDF\"):\n            self.add_doc(doc)\n\n    def predict(self, docs: List[str]) -> np.ndarray:\n        \"\"\"\n        Предсказывает TFIDF значения для списка документов.\n\n        :param docs: список документов\n        :return: матрица TFIDF значений\n        \"\"\"\n        matrix = []\n        for doc in tqdm(docs, desc=\"Computing prediction matrix\"):\n            token_ids = self.tokenizer.encode(doc)\n            counts = Counter(token_ids)\n            if self.tokenizer.unk_token_id in counts:\n                del counts[self.tokenizer.unk_token_id] # в задании сказано убрать <UNK> токен при подсчете tf-idf\n                \n            denom = sum(counts.values())\n            tf_idf_vector = np.zeros(self.vocab_size)\n\n            for token_id, count in counts.items():\n                tf = 0 if denom == 0 else count / denom # если все токены <UNK>, там есть такой assert\n                idf = self.idf(token_id)\n                tf_idf_vector[token_id] = tf * idf\n\n            if tf_idf_vector.sum() > 0:\n                tf_idf_vector /= tf_idf_vector.sum() # нормализация строк\n            matrix.append(tf_idf_vector)\n        \n        return np.array(matrix)\n            \n\n    def idf(self, term: int) -> float:\n        \"\"\"\n        Вычисляет IDF (обратную частоту документа) для термина.\n\n        :param term: термин\n        :return: IDF значение\n        \"\"\"\n        # тестовый корпус из ячейки ниже содержит всего 2 текста\n        # токен \"test\" появляется в обоих и в классической формуле\n        # idf(\"test\") == log(2 / 2) == 0\n        # поэтому в домашнем задании рекомендуется использовать\n        # сглаженный вариант подсчёта inverse document frequency smooth:\n        # log(N / (n + 1)) + 1\n        # https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Inverse_document_frequency\n        if self.term2num_docs[term] == 0:\n            return self.default_idf # если при обучении tf-idf не встретился такой токен\n        n_t = self.term2num_docs[term]\n        return np.log(self.num_docs / (1 + n_t)) + 1","metadata":{"id":"JlbuIZkoa_YP","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:39:10.489984Z","iopub.execute_input":"2025-01-30T14:39:10.490415Z","iopub.status.idle":"2025-01-30T14:39:10.502787Z","shell.execute_reply.started":"2025-01-30T14:39:10.490369Z","shell.execute_reply":"2025-01-30T14:39:10.501272Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"Тесты были проверены для такой комбинации формул:\n\n$$\n\\text{TF} = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}\n$$\n$$\n\\text{IDF} = \\log{\\frac{N}{1 + n_t}} + 1\n$$\n\nЕсли вы выбрали другие формулы для подсчёта, то можно поправить тесты соответственно.\n\n> Можно расширить расширить класс для удобства и поэксперементировать с различными формулами для подсчёта TF и IDF при классификации IMDB датасета ниже.","metadata":{"id":"PgAQTDUqa_YQ"}},{"cell_type":"code","source":"corpus = [\"test test\", \"not a test\"]\ntokenizer = Tokenizer(corpus)\ntfidf = TFIDF(tokenizer)\ntfidf.fit(corpus)","metadata":{"id":"tSm34Q1qa_YR","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:39:14.375609Z","iopub.execute_input":"2025-01-30T14:39:14.376010Z","iopub.status.idle":"2025-01-30T14:39:14.446642Z","shell.execute_reply.started":"2025-01-30T14:39:14.375974Z","shell.execute_reply":"2025-01-30T14:39:14.445186Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing texts:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1715001368924d3981578c4752e3a4da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Normalizing tokens:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d4beefebb9b4fe4a6e95ec486cd2a77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02d0140203cd4c629a88fa792877687b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training TF-IDF:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5246a872fe9f4bc196c6bf4243dd9ffe"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"assert tfidf.vocab_size == 4 + 3\n# 3 токена, один из которых <UNK>\nvector = tfidf.predict([\"a test string\"])[0]\n# tf(\"a\") == tf(\"test\") and idf(\"a\") > idf(\"test\")\nassert vector[tfidf.tokenizer.word2idx[\"a\"]] > vector[tfidf.tokenizer.word2idx[\"test\"]]\n\nvector = tfidf.predict([\"not a test a string\"])[0]\nassert vector[tfidf.tokenizer.word2idx[\"a\"]] > 2 * vector[tfidf.tokenizer.word2idx[\"test\"]]\nassert vector[tfidf.tokenizer.word2idx[\"a\"]] == 2 * vector[tfidf.tokenizer.word2idx[\"not\"]]\n\nassert not np.any(tfidf.predict([\"all tokens abscent from vocab should be zeros vector\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:39:17.286697Z","iopub.execute_input":"2025-01-30T14:39:17.287058Z","iopub.status.idle":"2025-01-30T14:39:17.342835Z","shell.execute_reply.started":"2025-01-30T14:39:17.287030Z","shell.execute_reply":"2025-01-30T14:39:17.341627Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Computing prediction matrix:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a86270bc64c143d39cef0f341d9265d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing prediction matrix:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11e6e224479148ae92aa5402001d76f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing prediction matrix:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ea74c0293c479198b1471b62dfc772"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"### Датасет (1 балл)\n\nВ качестве датасета вёзмём популярный [набор отзывов на фильмы с сайта IMDB](https://huggingface.co/datasets/stanfordnlp/imdb). Нужно предсказать является ли отзыв позитивным или негативным. Чтобы скачать датасет воспользуемся библиотекой `datasets` из экосистемы `HuggingFace`. Интерфейс датасета похож на словарь, доступ к разным частям осуществляется по названию ключа:\n\n1. Тренировочная часть датасета: `imdb[\"train\"]`\n2. Тексты для тренировки: `imdb[\"train\"][\"text\"]`\n3. Лейблы для тренировки: `imdb[\"train\"][\"label\"]`","metadata":{"id":"Ubkf1AHca_YR"}},{"cell_type":"code","source":"from datasets import load_dataset\n\n\nimdb = load_dataset(\"imdb\")","metadata":{"id":"quURkHUCa_YS","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:39:20.357646Z","iopub.execute_input":"2025-01-30T14:39:20.358018Z","iopub.status.idle":"2025-01-30T14:39:27.731862Z","shell.execute_reply.started":"2025-01-30T14:39:20.357987Z","shell.execute_reply":"2025-01-30T14:39:27.730363Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### Тренируем Токенайзер (2 балла)\n\nИспользуя `\"train\"` часть датасета, инициализируйте два токеназйера:\n1. Не использующий нормализацию\n2. Использующий функцию `normalize`, определённую выше\n\nСравним размер полученного словаря в обоих случаях.","metadata":{"id":"qNNgE-Osa_YS"}},{"cell_type":"code","source":"tokenizer_without_norm = Tokenizer(imdb[\"train\"][\"text\"])\ntokenizer_with_norm = Tokenizer(imdb[\"train\"][\"text\"], normalize_fn=normalize)\n\nassert len(tokenizer_without_norm) > len(tokenizer_with_norm)","metadata":{"id":"KefPmNzTa_YT","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:39:31.131400Z","iopub.execute_input":"2025-01-30T14:39:31.131961Z","iopub.status.idle":"2025-01-30T14:42:45.853971Z","shell.execute_reply.started":"2025-01-30T14:39:31.131929Z","shell.execute_reply":"2025-01-30T14:42:45.852430Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing texts:   0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4b54f7e1c60418ba4f4280e06fb4f37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Normalizing tokens:   0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10307a60c6f047728dcc43f1fa625b25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/7056536 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47deda97d404fedb96c1536efbd2efa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing texts:   0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef9ac69b4b5149a7950ff2c5673818ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Normalizing tokens:   0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22d5dc15b6ca419ba499ed0d369519c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/7056536 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0901d0f78ef472c91dfb6d8b5a80aa5"}},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"### Тренируем TF-IDF Модель (2 балла)\n\nТеперь мы можем натренировать модель на датасете. Обучим две модели, которые будут использовать разные токенайзеры.","metadata":{"id":"FRFQHmEha_YT"}},{"cell_type":"code","source":"tfidf_with_norm = TFIDF(tokenizer_with_norm)\ntfidf_with_norm.fit(imdb[\"train\"][\"text\"])","metadata":{"id":"tDt1os0Ra_YT","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:42:49.539216Z","iopub.execute_input":"2025-01-30T14:42:49.539650Z","iopub.status.idle":"2025-01-30T14:46:42.954717Z","shell.execute_reply.started":"2025-01-30T14:42:49.539604Z","shell.execute_reply":"2025-01-30T14:46:42.953434Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training TF-IDF:   0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa0c684a5c62492086a010e1eaaf45e2"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"tfidf_without_norm = TFIDF(tokenizer_without_norm)\ntfidf_without_norm.fit(imdb[\"train\"][\"text\"])","metadata":{"id":"4bBLeOCga_YT","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:46:45.693139Z","iopub.execute_input":"2025-01-30T14:46:45.693606Z","iopub.status.idle":"2025-01-30T14:47:40.494269Z","shell.execute_reply.started":"2025-01-30T14:46:45.693563Z","shell.execute_reply":"2025-01-30T14:47:40.492725Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training TF-IDF:   0%|          | 0/25000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34e12630258f4388818c13ddce54bd57"}},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### Обучим Логистическую Регрессию (5 баллов)\n\nВ качестве входов в модель нужно использовать TF-IDF представления документов (`X_train`), в качестве лейблов - 0 и 1, обозначающие нужный класс (`Y_train`). Начнём с модели, которая использует нормализацию.\n\nИспользуюя тестовый датасет и `logreg.predict` проверьте предсказания модели, вычислив accuracy - количество правильных предсказаний, делённое на количество входных примеров.","metadata":{"id":"8bEllErsa_YU"}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# в imdb[\"train\"] сначала идут 12500 нулей, а затем 12500 единиц\n# мне не хватало оперативы в kaggle, поэтому я чуть уменьшил датасет\nX_train = tfidf_with_norm.predict(imdb[\"train\"][\"text\"][10000:15000]) \nY_train = imdb[\"train\"][\"label\"][10000:15000] ","metadata":{"id":"jS-CUYIqa_YU","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:52:35.070209Z","iopub.execute_input":"2025-01-30T14:52:35.070654Z","iopub.status.idle":"2025-01-30T14:53:34.314033Z","shell.execute_reply.started":"2025-01-30T14:52:35.070606Z","shell.execute_reply":"2025-01-30T14:53:34.312707Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Computing prediction matrix:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84550b12d7624997942f8ad9fced7cc1"}},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)","metadata":{"id":"jS-CUYIqa_YU","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:53:34.315401Z","iopub.execute_input":"2025-01-30T14:53:34.315748Z","iopub.status.idle":"2025-01-30T14:53:38.757293Z","shell.execute_reply.started":"2025-01-30T14:53:34.315719Z","shell.execute_reply":"2025-01-30T14:53:38.756026Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"LogisticRegression()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# тут тоже чуть уменьшил тестовый сет\nX_test = tfidf_with_norm.predict(imdb[\"test\"][\"text\"][11000:14000])\nY_test = imdb[\"test\"][\"label\"][11000:14000]\n\npreds = logreg.predict(X_test)\naccuracy = (preds == Y_test).sum() / 3000\nprint(\"Accuracy: \", accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:54:51.032536Z","iopub.execute_input":"2025-01-30T14:54:51.032943Z","iopub.status.idle":"2025-01-30T14:55:28.112591Z","shell.execute_reply.started":"2025-01-30T14:54:51.032908Z","shell.execute_reply":"2025-01-30T14:55:28.111088Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Computing prediction matrix:   0%|          | 0/3000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e94c2fbe5c41709eae9c8c6f41401f"}},"metadata":{}},{"name":"stdout","text":"Accuracy:  0.7493333333333333\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"Теперь обучим логистическую регрессию со второй TF-IDF моделью и сравним результаты:","metadata":{"id":"gZzsjbIra_YU"}},{"cell_type":"code","source":"X_train = tfidf_without_norm.predict(imdb[\"train\"][\"text\"][10000:15000])\nY_train = imdb[\"train\"][\"label\"][10000:15000]\n\nlogreg_without_norm = LogisticRegression()\nlogreg_without_norm.fit(X_train, Y_train)\n\nX_test = tfidf_without_norm.predict(imdb[\"test\"][\"text\"][11000:14000])\nY_test = imdb[\"test\"][\"label\"][11000:14000]\n\npreds = logreg_without_norm.predict(X_test)\naccuracy = (preds == Y_test).sum() / 3000\nprint(\"Accuracy: \", accuracy)","metadata":{"id":"eQdWHrAEa_YU","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:55:50.817413Z","iopub.execute_input":"2025-01-30T14:55:50.817890Z","iopub.status.idle":"2025-01-30T14:56:44.126256Z","shell.execute_reply.started":"2025-01-30T14:55:50.817853Z","shell.execute_reply":"2025-01-30T14:56:44.125061Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Computing prediction matrix:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c37f44df74e340f8bbfd78e7550bbbbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing prediction matrix:   0%|          | 0/3000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fdf0cb5b9104592a014595cc1409d0d"}},"metadata":{}},{"name":"stdout","text":"Accuracy:  0.7366666666666667\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# получилось ожидаемо чуть меньше, но если обучать на всем датасете, то в теории разница будет существеннее","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### (Опционально) TfidfVectorizer\n\nМожете изучть класс [TfidfVectorizer](https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) из библиотеки scikit-learn и сравнить его со своей имплементацией, обучив логистическую регрессию с его помощью.","metadata":{"id":"kvA3demDa_YV"}},{"cell_type":"markdown","source":"## $n$-граммные языковые модели (15 баллов)\n\n### Расширяем Токенайзер (3 балла)\n\nПеред созданием языковой модели, расширим токенизационный класс. Добавим два флага в сигнатуру метода `encode`, чтобы управлять добавлением служебных токенов во время токенизации. Существующий метод `decode` уже пропускает `<PAD>` токен, добавим флаг `skip_special_tokens` для пропуска всех специальных токенов.","metadata":{"id":"yohzLpgoa_YV"}},{"cell_type":"code","source":"class BoSTokenizerEoS(Tokenizer):\n    def encode(self, text: str, add_bos: bool = True, add_eos: bool = False) -> List[int]:\n        \"\"\"\n        Кодирование текста в набор индексов.\n\n        :param text: текст\n        :param add_bos: добавление begin-of-sentence токена в начало\n        :param add_eos: добавление end-of-sentence токена в конец\n        :return: набор индексов токенов\n        \"\"\"\n        tokens = ['<BOS>'] if add_bos else []\n        tokens += [self.normalize(token) for token in self.tokenize(text)]\n        tokens += ['<EOS>'] if add_eos else []\n        return [self.encode_word(word) for word in tokens]\n\n    def decode(self, input_ids: List[int], skip_special_tokens: bool = True) -> str:\n        \"\"\"\n        Декодирование набора индексов в текст. Вставляет пробел между декодированнми токенами.\n\n        :param input_ids: набор индексов токенов\n        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n        :return: текст\n        \"\"\"\n        condition = 4 if skip_special_tokens else 0\n        tokens = [self.idx2word[id] for id in input_ids if id >= condition] # не декодирует, если спец. токен\n        return ' '.join(tokens)","metadata":{"id":"aKbGQakxa_YW","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:56:56.000976Z","iopub.execute_input":"2025-01-30T14:56:56.001319Z","iopub.status.idle":"2025-01-30T14:56:56.008525Z","shell.execute_reply.started":"2025-01-30T14:56:56.001291Z","shell.execute_reply":"2025-01-30T14:56:56.007132Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"### Создаём NGram Модель (12 баллов)\n\nСоздайте класс `NGramLanguageModel` для построения n-граммной языковой модели. В этом задании вы можете как опираться на предложенную структуру модели, так и сделать свою имплементацию.\n\nПостроение модели:\n   - Создайте метод `_build_model`, который принимает список текстов `texts` и обновляет частоты n-грамм.\n   - Для каждого текста:\n     - Токенизируйте текст и добавьте токен `\"<EOS>\"` в конец.\n     - Для каждого токена:\n       - Определите префикс длиной `n-1`.\n       - Обновите частоты n-грамм и частоты префиксов.\n\nГенерация следующего токена:\n   - Создайте метод `generate_next_token`, который принимает префикс `prefix` и возвращает следующий токен.\n   - Преобразуйте префикс в кортеж.\n   - Получите распределение частот для префикса.\n   - Если распределение пустое, верните токен `\"<UNK>\"`.\n   - Верните токен с наибольшей частотой.\n\nАвтодополнение текста:\n   - Создайте метод `autocomplete`, который принимает текст `text` и максимальную длину `max_len`, и возвращает завершенный текст.\n   - Токенизируйте текст.\n   - Пока длина токенов меньше `max_len`:\n     - Определите префикс длиной `n-1`.\n     - Сгенерируйте следующий токен.\n     - Добавьте токен в список токенов.\n     - Если токен равен `\"<EOS>\"`, завершите генерацию.\n   - Декодируйте и верните текст.","metadata":{"id":"R5TxQBEUa_YW"}},{"cell_type":"code","source":"from collections import defaultdict\n\nclass NGramLanguageModel:\n    def __init__(self, n: int, tokenizer, texts: List[str]):\n        \"\"\"\n        Создание n-граммной языковой модели.\n\n        :param n: порядок n-грамм\n        :param tokenizer: токенизатор\n        :param texts: список текстов\n        \"\"\"\n        assert n >= 2\n        self.n = n\n        self.tokenizer = tokenizer\n        self.frequencies = defaultdict(lambda: Counter())  # частота n-грамм\n        self.frequencies_of_prefixes = Counter()  # сумма частот n-грамм для префиксов\n        self._build_model(texts)\n\n    def _build_model(self, texts: List[str]):\n        \"\"\"\n        Построение модели на основе списка текстов.\n        Заполнение частот префиксов и следующих за префиксами токенов.\n\n        :param texts: список текстов\n        \"\"\"\n        for text in texts:\n            tokens = self.tokenizer.encode(text, add_bos=False, add_eos=True)\n            for i in range(len(tokens) - self.n + 1):\n                prefix = tuple(tokens[i:i + self.n - 1])\n                token = tokens[i + self.n - 1]\n                self.frequencies[prefix][token] += 1\n                self.frequencies_of_prefixes[prefix] += 1\n\n    def generate_next_token(self, prefix: List[int]) -> int:\n        \"\"\"\n        Жадная генерация следующего токена по префиксу.\n\n        :param prefix: префикс\n        :return: следующий токен\n        \"\"\"\n        prefix_tuple = tuple(prefix[-(self.n - 1):]) # на основе последних n - 1 токена\n        if prefix_tuple not in self.frequencies:\n            return self.tokenizer.unk_token_id\n        return max(self.frequencies[prefix_tuple], key=self.frequencies[prefix_tuple].get)\n\n    def autocomplete(self, text: str, max_len: int = 32, skip_special_tokens: bool = True) -> str:\n        \"\"\"\n        Автоматическое дополнение текста.\n\n        :param text: текст\n        :param max_len: максимальная длина текста\n        :param skip_special_tokens: пропуск специальных токенов во время декодирования.\n        :return: завершенный текст\n        \"\"\"\n        tokens = self.tokenizer.encode(text)\n        for _ in range(max_len - len(tokens)):\n            prefix = tokens[-(self.n - 1):]\n            next_token = self.generate_next_token(prefix)\n            tokens.append(next_token)\n            if next_token == self.tokenizer.word2idx['<EOS>']:\n                break\n        return self.tokenizer.decode(tokens, skip_special_tokens=skip_special_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:57:00.128959Z","iopub.execute_input":"2025-01-30T14:57:00.129312Z","iopub.status.idle":"2025-01-30T14:57:00.141802Z","shell.execute_reply.started":"2025-01-30T14:57:00.129282Z","shell.execute_reply":"2025-01-30T14:57:00.140713Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"corpus = [\"Hello, world!\", \"I love Python!\", \"Hello, Python\"]\ntokenizer = BoSTokenizerEoS(corpus, min_count=1)","metadata":{"id":"Z1YwmEija_YX","trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:57:04.817780Z","iopub.execute_input":"2025-01-30T14:57:04.818156Z","iopub.status.idle":"2025-01-30T14:57:04.875114Z","shell.execute_reply.started":"2025-01-30T14:57:04.818121Z","shell.execute_reply":"2025-01-30T14:57:04.873800Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing texts:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13bfa56578154919a587c37e3cb3fd23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Normalizing tokens:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a8d1a0fe53149ea87f46bc1e9d32cb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b08e0ceb9dc411ab331a66112d0a30e"}},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"ngram_lm = NGramLanguageModel(2, tokenizer, corpus)\nassert ngram_lm.autocomplete(\"Hello, Python\", max_len=10) == \"Hello , Python !\"\nassert ngram_lm.autocomplete(\"Hello, Python\", max_len=10, skip_special_tokens=False) == \"<BOS> Hello , Python ! <EOS>\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T14:57:07.743828Z","iopub.execute_input":"2025-01-30T14:57:07.744426Z","iopub.status.idle":"2025-01-30T14:57:07.754528Z","shell.execute_reply.started":"2025-01-30T14:57:07.744365Z","shell.execute_reply":"2025-01-30T14:57:07.752788Z"}},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":"# Комментарии\n\nЕсли остались вопросы, на которые хочется получить ответ при ревью, это место для них:","metadata":{"id":"_xhNRmSZa_YX"}},{"cell_type":"markdown","source":"","metadata":{"id":"Gp-uq1kza_YX"}}]}