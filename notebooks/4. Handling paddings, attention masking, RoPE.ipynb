{"cells":[{"cell_type":"markdown","metadata":{"id":"aFd4gLTldVdQ","jp-MarkdownHeadingCollapsed":true},"source":["## Скачиваем данные"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WLniBU_BRRq"},"outputs":[],"source":["! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2YsvKXSBRRr"},"outputs":[],"source":["import sys\n","import einops\n","import torch\n","import torch as t\n","from torch import Tensor\n","import torch.nn as nn\n","import numpy as np\n","import math\n","from tqdm.notebook import tqdm\n","from typing import Tuple, List, Optional, Dict, Callable\n","from jaxtyping import Float, Int\n","from transformers import AutoTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mSO6yzmBBRRr"},"outputs":[],"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"HGetEHfUBRRs"},"source":["# Подготовка данных\n","\n","У нас есть тексты пьесы Шекспира"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DsXrLEIFBRRs"},"outputs":[],"source":["with open(\"input.txt\") as fin:\n","    text = fin.read()\n","\n","print(text[:200])"]},{"cell_type":"markdown","metadata":{"id":"5-wfNDmSBRRs"},"source":["Создаем токенайзер, обратите внимание, что у токена there должен быть вначале спецсимвол, обозначающий, что это новое слово, а не часть предыдущего! Используем модель `openai-community/gpt2`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mUS68aMVBRRs"},"outputs":[],"source":["tokenizer = AutoTokenizer ... # Допишите меня!\n","assert tokenizer.tokenize(\"Hello there sometrashtoken\") == ['Hello', 'Ġthere', 'Ġsomet', 'r', 'ash', 'token']\n","assert tokenizer.eos_token == \"<|endoftext|>\""]},{"cell_type":"markdown","metadata":{"id":"z-jNhR_7BRRs"},"source":["В токенайзере нет спецтокена под паддинг, поэтому выставим PAD_TOKEN = EOS_TOKEN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFc1mHyhBRRs"},"outputs":[],"source":["tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.pad_token_id = tokenizer.eos_token_id"]},{"cell_type":"markdown","metadata":{"id":"BlBJ9oi5BRRt"},"source":["# Датасет - 5 баллов\n","\n","Нам нужен Dataset - что-то, что будет держать данные.\n","Почитать подробнее можно в [документации](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) или на [примерах](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html).\n","\n","\n","Если кратко:\n","* Dataset должен реализовывать 2 метода: `__getitem__` для получения сэмплов и `__len__` для получения длины датасета\n","* Нужна функция collate_fn - она будет собирать несколько сэмплов из датасета в один батч\n","* Нужен DataLoader - объект, который будет брать объекты из датасета и с помощью collate_fn возвращать батчи\n","* Нужен Sampler - объект, который помогает DataLoader выбирать батчи. В нашем случае это будет просто рандом, но можно собирать сэмплы по одинаковой длине или упорядочить в зависимости от задачи.\n","\n","\n","Начнем с Dataset. В нем нужно дописать 3 функции, самая важная конструктор `__init__`:\n","1. Принимает текст\n","2. Токенизирует его\n","3. Бьет на непересекающиеся сэмплы размером 200-300 токенов (длину определяем с помощью random.randint)\n","3. Кладет токены (векторизированные!) в self.texts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4n6zkkjCBRRt"},"outputs":[],"source":["from typing import List\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","class MyDataset(Dataset):\n","\n","    def __init__(self, tokenizer: AutoTokenizer, text: str):\n","        self.tokenizer = tokenizer\n","        self.texts: List[List[int]] = []\n","        random.seed(1)\n","        # YOUR CODE HERE\n","        # Заполните массив texts!\n","        # нужно запол\n","\n","\n","    def __getitem__(self, index) -> List[int]:\n","        ... # ВАШ КОД ЗДЕСЬ\n","\n","\n","    def __len__(self) -> int:\n","        ... # Ваш код здесь!\n","\n","\n","dataset = MyDataset(tokenizer, text)\n","\n","sample_0 = dataset.tokenizer.decode(dataset[0])\n","\n","assert sample_0.startswith(text[:100])\n","\n","print(sample_0)"]},{"cell_type":"markdown","metadata":{"id":"NEmTxrbMBRRt"},"source":["# Collate FN - 5 баллов\n","Функция сборки, она же collate_fn. Она принимает батч сэмплов, т.е. список объектов, которые нам возвращает датасет!\n","Она должна принимать `List[List[int]]` батч объектов и возвращать 2 тензора:\n","\n","* input_ids - `[batch, seq_len]` - батч токенов, в котором добавлены паддинги до максимальной длины в батче.\n","* mask - `[batch, seq_len]` - батч масок. На позиции `[i, j]` стоит 0, если токен является паддингом, иначе 1.\n","\n","В качестве значения паддинга для input_ids используйте `tokenizer.pad_token_id`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rjlv3nwOBRRt"},"outputs":[],"source":["def collate_fn(batch: List[List[int]]) -> Tuple[torch.LongTensor, torch.LongTensor]:\n","    # Ваш код здесь\n","    ...\n","\n","\n","\n","batch = [\n","    [1, 2, 3, 4],\n","    [1, 2],\n","    [1, 2, 3, 4, 5, 6, 7],\n","]\n","input_ids_ref = torch.LongTensor([\n","    [1, 2, 3, 4, 50256, 50256, 50256],\n","    [1, 2, 50256, 50256, 50256, 50256, 50256],\n","    [1, 2, 3, 4, 5, 6, 7],\n","])\n","\n","\n","mask_ref = torch.LongTensor([\n","    [1, 1, 1, 1, 0, 0, 0],\n","    [1, 1, 0, 0, 0, 0, 0],\n","    [1, 1, 1, 1, 1, 1, 1],\n","])\n","\n","input_ids, mask = collate_fn(batch)\n","\n","assert (input_ids == input_ids_ref).all()\n","assert (mask == mask_ref).all()"]},{"cell_type":"markdown","metadata":{"id":"vYflR287BRRt"},"source":["# Соберем DataLoader - 5 баллов\n","\n","Нужно заполнить пропущенные поля и убедиться, что в датасете есть замаскированные токены!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVc4jTbaBRRt"},"outputs":[],"source":["from torch.utils.data.sampler import RandomSampler\n","\n","BATCH_SIZE = 16\n","sampler = ...\n","train_loader = DataLoader(\n","    dataset=...,\n","    batch_size=...,\n","    shuffle=False,\n","    sampler=...,\n","    collate_fn=..,\n",")\n","\n","for input_ids, mask in train_loader:\n","    break\n","print(mask)\n","\n","assert (mask.sum(dim=1) < mask.size(1)).sum() < mask.size(0)"]},{"cell_type":"markdown","metadata":{"id":"pblCA3rzBRRt"},"source":["# Transformer\n","\n","Немного модфицированный блок трансформера, который мы скопируем с предыдущего занятия!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fUVcxvU3BRRt"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from dataclasses import dataclass\n","\n","\n","@dataclass\n","class Config:\n","    d_model: int = 768 # он же hidden_dim - внутрення размерность модели\n","    debug: bool = True\n","    layer_norm_eps: float = 1e-5\n","    d_vocab: int = 50257 # он же vocab_size, размер словаря модели\n","    init_range: float = 0.02\n","    n_ctx: int = 1024 # число позиционных эмбеддингов\n","    d_head: int = 64 # размерность головы аттеншена\n","    d_mlp: int = 3072 # внутренняя размерность FFN-слоя\n","    n_heads: int = 12 # число голов аттеншена\n","    n_layers: int = 12 # число слоев трансформера\n","\n","cfg = Config()\n","print(cfg)"]},{"cell_type":"markdown","metadata":{"id":"aGQuKP1UBRRt"},"source":["Эти модули остаются без изменений!\n","Скопируйте их из предыдущего домашнего задания."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vzvGv_8GBRRu"},"outputs":[],"source":["class Embed(nn.Module):\n","    def __init__(self, cfg: Config):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n","        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n","\n","    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n","        # Скопируйте меня!\n","        ...\n","\n","class PosEmbed(nn.Module):\n","    def __init__(self, cfg: Config):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n","        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n","\n","    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n","        # Скопируйте меня!\n","        ...\n","\n","\n","class Unembed(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n","        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n","        self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n","\n","    def forward(\n","        self, x: Float[Tensor, \"batch seq_len d_model\"]\n","    ) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n","        # Скопируйте меня!\n","        ...\n","\n","class MLP(nn.Module):\n","    def __init__(self, cfg: Config):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n","        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n","        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n","        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n","        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n","        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n","\n","    def forward(\n","        self, x: Float[Tensor, \"batch seq_len d_model\"]\n","    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n","        # Скопируйте меня!\n","        ...\n","\n"]},{"cell_type":"markdown","metadata":{"id":"M0ef4RpFBRRu"},"source":["# RMSNorm - 5 баллов\n","https://arxiv.org/pdf/1910.07467"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mE4tCrSpBRRu"},"outputs":[],"source":["class RMSNorm(nn.Module):\n","    def __init__(self, cfg: Config):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.w = nn.Parameter(t.ones(cfg.d_model)) # gamma\n","\n","    def forward(self, x: Float[Tensor, \"batch seq_len d_model\"]) -> Float[Tensor, \"batch seq_len d_model\"]:\n","        # Напишите меня!\n","        ...\n","\n","\n","\n","cfg_rmsnorm = Config(d_model=5)\n","x = torch.Tensor([[[0.1, 0.2, 0.3, 0.4, 0.5]]])\n","layer = RMSNorm(cfg_rmsnorm)\n","y = torch.Tensor([[[0.3015, 0.6030, 0.9045, 1.2060, 1.5076]]])\n","assert torch.allclose(y, layer(x), atol=1e-4, rtol=1e-3)"]},{"cell_type":"markdown","metadata":{"id":"r8AkTMAmBRRu"},"source":["# Attention"]},{"cell_type":"markdown","metadata":{"id":"VGqLthbPBRRu"},"source":["## Attention Masking - 10 баллов\n","\n","Опять же копируем имлементацию из предыдущего кода.\n","Но теперь нужно учесть и маски с паддингами.\n","Для этого в `apply_causal_mask` подана mask.\n","\n","В оригинальном задании 3 мы считали, что паддингов нет, поэтому делали маску нижней треугольной, чтобы токен i смотрел на токен j только тогда, когда `i >= j`, т.е. токен i мог смотреть все токены до него.\n","\n","Теперь же нужно сверх этого добавить еще и паддинг, т.е:\n","\n","1. Нам дается маска `[batch_size, seq_len]` из `collate_fn`. Напомню, что на позиции `[batch_idx, m]` стоит 1, если токен настоящий или 0, если это паддинг\n","2. Мы должны модифицировать нашу нижнюю треугольную маску таким образом, чтобы не только не смотреть в будущее, но и не смотреть на паддинг."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMJp8ZJQBRRu"},"outputs":[],"source":["class Attention(nn.Module):\n","    IGNORE: Float[Tensor, \"\"]\n","\n","    def __init__(self, cfg: Config):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n","        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n","\n","        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n","        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n","\n","        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n","        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n","\n","        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n","        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n","\n","        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n","        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n","        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n","        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n","        self.register_buffer(\"IGNORE\", t.tensor(float(\"-inf\"), dtype=t.float32, device=device))\n","\n","    def forward(\n","        self, x: Float[Tensor, \"batch seq_len d_model\"], mask: Int[Tensor, \"batch seq_len\"]\n","    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n","        # меня нужно скопировать, но пробросить аргумент mask\n","        # в apply_causal_mask!\n","\n","    def apply_causal_mask(\n","        self, attn_scores: Float[Tensor, \"batch n_heads seq_len seq_len\"], mask: Int[Tensor, \"batch seq_len\"]\n","    ) -> Float[Tensor, \"batch n_heads seq_len seq_len\"]:\n","        '''\n","        Applies a causal mask to attention scores, and returns masked scores.\n","        Используем треугольную маску, чтобы не смотреть в будущее!\n","        В качестве масикировочного значения перед софтмаксом можно использовать self.IGNORE (-inf)\n","        '''\n","        ...\n","        # В меня нужно дописать превращение маски по паддингам в\n","        # causal mask!\n","\n","\n","mask_padding = torch.LongTensor([\n","    [1, 1, 1, 1, 0, 0, 0],\n","    [1, 1, 0, 0, 0, 0, 0],\n","    [1, 1, 1, 1, 1, 1, 1],\n","])\n","\n","lengths = mask_padding.sum(dim=1).tolist()\n","\n","\n","batch_size = 3\n","seq_len = 7\n","d_head = 8\n","n_heads = 4\n","\n","x = torch.rand(batch_size, n_heads, seq_len, seq_len)\n","\n","attn = Attention(cfg)\n","softmax_res = torch.softmax(attn.apply_causal_mask(x, mask_padding), dim=-1)\n","\n","for batch_idx in range(batch_size):\n","    for head_idx in range(n_heads):\n","        sm = softmax_res[batch_idx, head_idx]\n","        l = lengths[batch_idx]\n","        for i in range(seq_len):\n","            for j in range(seq_len):\n","                # i < j - Causal mask, проверяем, что не смотрим в будущее!\n","                # j >= l - проверяем, что не смотрим на паддинги!\n","                if i < j or j >= l:\n","                    assert sm[i, j] == 0, (batch_idx, head_idx, i, j, sm[i, j])\n","\n","_ = attn(torch.rand(batch_size, seq_len, 768), mask_padding)"]},{"cell_type":"markdown","metadata":{"id":"nq0EcySUBRRu"},"source":["## Rotary Embeddings - 5 баллов\n","\n","Нужно написать роторные эмбеддинги из [статьи](https://arxiv.org/pdf/2104.09864). В качестве формулы нужно взять пункт 3.4.2!\n","\n","Их можно использовать в attention, но это не обязательно, здесь баллы даются только за имплементацию самих роторных эмбеддингов.\n","\n","\n","Что нужно сделать в `__init__` (все нужно делать опираясь на формулу 34 из статьи):\n","1. Сделать 2 матрицы $cos(m\\theta_i)$, $sin(m\\theta_i$. Это будут матрицы размера `[max_seq_len, d_head]`\n","2. Берем от матрицы синусы, косинусы.\n","3. Дальше мы их добавляем в модель через register_buffer. Для удобства сразу делаем фиктивные доп размерности для batch_size, num_heads.\n","\n","\n","Что нужно сделать в `rotate_neg_vector`:\n","\n","1. По формуле 34 вернуть по вектору `[x1, x2, x3, x4, ... x_{n-1}, x_n]` новый вектор `[-x2, x1, -x4, x3, ..., -x_n, x_{n-1}]`\n","\n","\n","Что нужно сделать в `forward`:\n","1. Получить вектор x' `[-x2, x1, -x4, x3, ..., -x_n, x_{n-1}]`\n","2. Применить для x и x' формулу 34"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bY07Be_MBRRu"},"outputs":[],"source":["class RotaryPositionalEmbeddings(nn.Module):\n","\n","    def __init__(self, cfg: Config, theta: int = 10_000):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.max_seq_len = cfg.n_ctx\n","        self.theta = theta\n","        self.d = cfg.d_head\n","\n","\n","        # Углы theta_i. Смотрите секуцию 2.2 статьи для формулы!\n","        freqs = ...\n","        position_id = torch.arange(0, self.max_seq_len).float()\n","\n","        # нужно получить матрицу m theta_i размера [max_seq_len, self.d] вида m theta_i\n","        # где m берется из position_id, а theta из freqs\n","\n","        idx_theta = ..\n","\n","        # max_seq_len, d_head\n","        cos = idx_theta.cos()\n","        sin = idx_theta.sin()\n","\n","        # нужно продублировать размерности для формулы 34. theta_i встерчается два раза подряд в синусах и косинуса\n","        # тут нам поможет torch.repeat_interleave\n","        cos = ...\n","        sin = ...\n","\n","        # 1, max_seq_len, 1, d_head\n","        self.register_buffer(\"sin\", sin.view(1, self.max_seq_len, 1, self.d))\n","        self.register_buffer(\"cos\", cos.view(1, self.max_seq_len, 1, self.d))\n","\n","    @staticmethod\n","    def rotate_neg_vector(x: Float[torch.Tensor, \"batch seq_len num_heads d_head\"]):\n","        # На входе x = [x1, x2, x3, x4, ... x_{n-1}, x_n]\n","        # На выходе x' = [-x2, x1, -x4, x3, ..., -x_n, x_{n-1}]\n","        x_new = torch.empty_like(x)\n","        ...\n","        return x_new\n","\n","    def forward(self, x: Float[torch.Tensor, \"batch seq_len num_heads d_head\"]):\n","        seq_len = x.size(1)\n","        ...\n","        return ...\n","\n","\n","\n","\n","batch_size = 1\n","seq_len = 3\n","num_heads = 2\n","d_head = 16\n","\n","torch.manual_seed(1)\n","x = torch.rand(batch_size, seq_len, num_heads, d_head)\n","\n","rope_config = Config(\n","    n_heads=2,\n","    d_head=16,\n",")\n","\n","rope_layer = RotaryPositionalEmbeddings(rope_config)\n","y = rope_layer(x)\n","\n","\n","from math import sin, cos\n","\n","\n","thetas = [10_000 ** (-2 * (i - 1) / rope_config.d_head) for i in range(1, rope_config.d_head // 2 + 1)]\n","all_good = True\n","for batch_idx in range(batch_size):\n","    for m in range(seq_len):\n","        if not all_good:\n","            break\n","        for head_idx in range(num_heads):\n","            if not all_good:\n","                break\n","            for d_idx in range(d_head):\n","                # 0, 2, 4\n","                if d_idx % 2 == 0:\n","                    val = x[batch_idx, m, head_idx, d_idx] * cos(m * thetas[d_idx // 2]) - x[batch_idx, m, head_idx, d_idx + 1] * sin(m * thetas[d_idx // 2])\n","                else:\n","                    val = x[batch_idx, m, head_idx, d_idx] * cos(m * thetas[d_idx // 2]) + x[batch_idx, m, head_idx, d_idx - 1] * sin(m * thetas[d_idx // 2])\n","                if abs(y[batch_idx, m, head_idx, d_idx] - val) > 1e-3:\n","                    print(f\"Ошибка на позиции {m} и размерности {d_idx} в голове {head_idx}\")\n","                    print(f\"Полученное значение {y[batch_idx, m, head_idx, d_idx]}, референс {val}\")\n","                    all_good = False\n","                    break\n","\n","\n","if all_good:\n","    print(\"Тесты прошли успешно!\")\n"]},{"cell_type":"markdown","metadata":{"id":"AtT6Bq1_BRRu"},"source":["# Rope X Attention\n","\n","Если хотите обучаться с RoPE вместо позиционных эмбеддингов, то ниже можете написать новый класс Attention, где будете \"вращать\" Q, K. Не забудьте только дальше в DemoTransformer убрать позиционные эмбеддинги\n","\n","**В независимости от вашего решения напишите ответ на вопрос.**\n","\n","В модели есть RoPE-слой. theta = 10.000, максимальное число позиций 1024. Размерность головы (размерность RoPE для простоты 16). Какое число обучаемых параметров в RoPE-слое?\n","\n","Ваш ответ:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-rB1jPBBRRu"},"outputs":[],"source":["# ОПЦИОНАЛЬНО\n","class AttentionWithRope():\n","    ..."]},{"cell_type":"markdown","metadata":{"id":"STRBq2mJBRRu"},"source":["# Собираем Transformer - 5 баллов\n","\n","1. В TransformerBlock и DemoTransformer немного модифицируем код из предыдущего задания, чтобы передавать mask в слои аттеншена.\n","2. В зависимости от того, хотим ли мы использовать RoPE или не хотим меняется также то, используем ли мы Positional Embeddings или нет!\n","\n","В задании не будут сниматься баллы, если используются абсолютные позиционные эмбеддинги.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Nt798OABRRu"},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, cfg: Config):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.ln1 = RMSNorm(cfg)\n","        self.attn = Attention(cfg)\n","        self.ln2 = RMSNorm(cfg)\n","        self.mlp = MLP(cfg)\n","\n","    def forward(\n","        self, x: Float[Tensor, \"batch seq_len d_model\"], mask: Float[Tensor, \"batch seq_len\"]\n","    ) -> Float[Tensor, \"batch seq_len d_model\"]:\n","        attn = self.attn(self.ln1(x), mask)\n","        ...\n","        # Ваш код здесь!\n","\n","class DemoTransformer(nn.Module):\n","    def __init__(self, cfg: Config):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.embed = Embed(cfg)\n","        self.pos_embed = PosEmbed(cfg)\n","        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n","        self.ln_final = RMSNorm(cfg)\n","        self.unembed = Unembed(cfg)\n","\n","    def forward(self, input_ids: Int[Tensor, \"batch seq_len\"], mask: Int[Tensor, \"batch seq_len\"]) -> Float[Tensor, \"batch seq_len d_vocab\"]:\n","        ...\n","        # Ваш код здесь!"]},{"cell_type":"markdown","metadata":{"id":"F0Kqa1Y2BRRv"},"source":["# Финальные проверки"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"41ob14g-BRRv"},"outputs":[],"source":["train_config = Config(\n","    d_model=128,\n","    n_ctx=512,\n","    n_heads=8,\n","    d_head=16,\n","    d_mlp=512,\n","    n_layers=12\n",")\n","model = DemoTransformer(train_config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWDc9s8yBRRv"},"outputs":[],"source":["for input_ids, mask in train_loader:\n","    break\n","\n","with torch.no_grad():\n","    p = model(input_ids, mask)\n","\n","assert list(p.shape) == [input_ids.size(0), input_ids.size(1), train_config.d_vocab]\n"]},{"cell_type":"markdown","metadata":{"id":"WpvPov13BRRv"},"source":["# Обучение - 5 баллов\n","\n","Здесь нужно написать обычный training loop! Но он не так уж и прост. Давайте по шагам разберемся, как нам быть:\n","1. Берем input_ids, mask, прогоняем через модель, получаем тензор logits `[batch_size, seq_len, vocab_size]`\n","2. В качестве меток мы берем input_ids! Только нужно их обрезать по размерности seq_len таким образом, чтобы i-й токен предсказывал (i + 1)-й!\n","\n","3. В качестве предиктов берем logits! Его тоже нужно правильно обрезать, т.к. по последнему токену нам нечего предсказывать.\n","\n","4. Паддингам ставим метки -100, это значение ignore_loss, [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) его игнорирует при подсчете лососв!\n","5. Превращаем p в тензор `[batch_size * (seq_len - 1), vocab_size]`, вектор правильных меток labels (из input_ids) превращаем в `[batch_size * (seq_len - 1)]`, считаем функцию потерь!\n","6. Не забываем все стандартные вещи в training loop: обнуление градиентов, backward, шаг оптимизации.\n"]},{"cell_type":"markdown","metadata":{"id":"3kMKYD2DBRRv"},"source":["Вначале напишем функцию потерь!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1lgcdkEwBRRv"},"outputs":[],"source":["from math import log\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","pad_id = 50256\n","\n","def calculate_loss(critertion, logits, input_ids, pad_id=pad_id):\n","    ...\n","\n","    return 0\n","\n","\n","\n","batch_size = 2\n","seq_len = 4\n","num_classes = 7\n","\n","input_ids = torch.LongTensor(\n","    [\n","        [0, 1,  pad_id, pad_id],\n","        [0, 1, 2, 3]\n","    ]\n",")\n","\n","\n","# batch_size, seq_len, num_classes\n","logits = torch.Tensor(\n","    [[[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999, 0.3971],\n","         [0.7544, 0.5695, 0.4388, 0.6387, 0.5247, 0.6826, 0.3051],\n","         [0.4635, 0.4550, 0.5725, 0.4980, 0.9371, 0.6556, 0.3138],\n","         [0.1980, 0.4162, 0.2843, 0.3398, 0.5239, 0.7981, 0.7718]],\n","\n","        [[0.0112, 0.8100, 0.6397, 0.9743, 0.8300, 0.0444, 0.0246],\n","         [0.2588, 0.9391, 0.4167, 0.7140, 0.2676, 0.9906, 0.2885],\n","         [0.8750, 0.5059, 0.2366, 0.7570, 0.2346, 0.6471, 0.3556],\n","         [0.4452, 0.0193, 0.2616, 0.7713, 0.3785, 0.9980, 0.9008]]]\n",")\n","\n","loss = calculate_loss(criterion, logits, input_ids)\n","\n","assert abs(loss.item() - 1.9343) < 1e-2"]},{"cell_type":"markdown","metadata":{"id":"8S0D8bxwBRRv"},"source":["А теперь с помощью `calculate_loss` напишем цикл ообучения"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuIQnuA1BRRv"},"outputs":[],"source":["import torch.optim as optim\n","\n","model = model.train()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","\n","losses = []\n","for epoch in range(10):\n","    for input_ids, mask in tqdm(train_loader):\n","        # Напишите сюда training loop!\n","        losses.append(loss.item())"]},{"cell_type":"markdown","metadata":{"id":"NTKygJkoBRRz"},"source":["Лоссы должны идти вниз"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADJfv8wEBRRz"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.plot(losses)"]},{"cell_type":"markdown","metadata":{"id":"MgACMkFHBRRz"},"source":["# Генерация - 5 баллов\n","Давайте теперь попробуем посмотреть, что у нас обучилось! Для этого проверим себя на жадной генерации. KV-cache не пишем, просто:\n","1. Подаем input_ids, mask\n","2. По последнему токену жадно предсказываем следующий\n","3. Конактенируем этот токен к input_ids, расширяем mask\n","4. Повторяем num_tokens_to_generate раз"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0OqUV3aBRRz"},"outputs":[],"source":["input_text = text[:5]\n","inputs = tokenizer(input_text, return_tensors=\"pt\")\n","\n","input_ids = inputs[\"input_ids\"]\n","mask = inputs[\"attention_mask\"]\n","\n","orig_size = input_ids.size(1)\n","\n","num_tokens_to_generate = 10\n","\n","with torch.no_grad():\n","    for i in range(num_tokens_to_generate):\n","        # Ваш код здесь\n","        ...\n","\n","print(\"Input text:\", input_text)\n","print(\"Input text + Generated\", tokenizer.decode(input_ids[0]))"]},{"cell_type":"markdown","metadata":{"id":"OCfETlijBRRz"},"source":["Если все прошло успешно, то мы увидим какой-то небольшой, но скорее всего повторяющийся текст.\n","\n","Осталось отмашстабировать модель, накинуть данных и наша LLM готова!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ORAv8lf9BRRz"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}